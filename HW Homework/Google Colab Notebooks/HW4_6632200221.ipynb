{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 4: Social Media Data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YTDQanrJ86Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‡∏à‡∏∏‡∏î‡∏°‡∏∏‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô**\n",
        "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
        "  - String and file processing\n",
        "  - Basic dict\n"
      ],
      "metadata": {
        "id": "mXvbUjnTOV2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##**‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô**\n",
        "1. ‡πÉ‡∏´‡πâ copy file ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Google Drive ‡∏Ç‡∏≠‡∏á‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏°‡∏ô‡∏π File->Save a copy in Drive\n",
        "2. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà copy ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô HW4_XXXXXXXXXX.ipynb ‡πÄ‡∏°‡∏∑‡πà‡∏≠ XXXXXXXXXX ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏•‡∏Ç‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß‡∏ô‡∏¥‡∏™‡∏¥‡∏ï\n",
        "\n",
        "## <font color=red>‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢\n",
        "## ‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°</font>\n",
        " - ‡∏´‡πâ‡∏≤‡∏° import ‡πÉ‡∏î ‡πÜ\n",
        " - ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà print ‡πÉ‡∏î ‡πÜ ‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\n",
        " - ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î `def` ‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô\n",
        " - ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
        " - ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö\n",
        " - ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á code cell ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ `# HW4_Social_Media_Data` ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏õ ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
        " - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤\n",
        " ```\n",
        " # Write your functions here ONLY (If any)\n",
        " ```\n",
        "\n",
        "- <font color=red>‡∏™‡πà‡∏≠‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï</font> ‡πÄ‡∏ä‡πà‡∏ô  \n",
        " - ‡∏™‡πà‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n",
        "  - ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡πà‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô‡∏°‡∏≤‡∏Å ‡πÜ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö ‡∏à‡∏∞‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πá‡∏ï‡∏≤‡∏°)\n",
        "  - ‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô\n",
        "    - ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ ‡πÑ‡∏°‡πà‡∏î‡∏π‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô\n",
        "    - ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏î‡∏π‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\n",
        "\n",
        "- ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏™‡πà‡∏≠‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏à‡∏∞‡πÑ‡∏î‡πâ $0$ ‡πÉ‡∏ô<font color=red>‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î</font>\n",
        "- ‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‡πÄ‡∏°‡∏∑‡πà‡∏≠\n",
        " - ‡πÅ‡∏ü‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏™‡∏∏‡∏î‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡πà‡∏á‡πÉ‡∏ô MyCourseVille ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ü‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ü‡πâ‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ File->Download->Download .ipynb ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏ü‡πâ‡∏° .py ‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏≤ rename ‡πÄ‡∏õ‡πá‡∏ô .ipynb)\n",
        " - ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡πà‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô code cell ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
        " - ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°\n",
        " - code cell ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ (‡πÑ‡∏°‡πà‡∏°‡∏µ error)\n",
        " - ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ code ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡πÑ‡∏°‡πà‡∏°‡∏µ syntax error, indentation error ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô code ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡∏Å‡πá‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÑ‡∏î‡πâ 0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DLvxhuA0NIzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô**\n"
      ],
      "metadata": {
        "id": "lgsq1C6oOkg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö .ipynb ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏°‡∏ô‡∏π File->Download->Download .ipynb ‡∏Ç‡∏≠‡∏á Colab ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÅ‡∏ü‡πâ‡∏° ipynb ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô MyCourseVille\n",
        "2. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô HW4_XXXXXXXXX.ipynb ‡πÄ‡∏°‡∏∑‡πà‡∏≠ XXXXXXXXXX ‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡∏•‡∏Ç‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß‡∏ô‡∏¥‡∏™‡∏¥‡∏ï\n",
        "3. ‡∏™‡πà‡∏á‡πÅ‡∏ü‡πâ‡∏° .ipynb ‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏ü‡πâ‡∏° .ipynb **‡πÅ‡∏ü‡πâ‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î** ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
        "4. <font color=\"red\">**‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡πà‡∏á ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡πà‡∏≠‡∏ô 23:59 ‡∏ô. ‡∏Ç‡∏≠‡∏á‡∏û‡∏∏‡∏ò‡∏ó‡∏µ‡πà 27 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2567**</font>"
      ],
      "metadata": {
        "id": "v7-Ib0uROq3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV File\n",
        "Comma-separated values ‡∏´‡∏£‡∏∑‡∏≠ CSV ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ comma (,) ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÑ‡∏î‡πâ\n",
        "\n",
        "* ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏≤‡∏á‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•‡∏°‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®\n",
        "* ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô colab ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\n",
        "* ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ô‡∏µ‡πâ‡∏Å‡πá‡πÑ‡∏î‡πâ https://drive.google.com/file/d/1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp/view?usp=share_link\n",
        "* ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Social Media Sentiments Analysis Dataset ‡∏ö‡∏ô Kaggle ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏ô‡∏µ‡πâ"
      ],
      "metadata": {
        "id": "aHC4vTRPWZtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp -O social_media_data.csv"
      ],
      "metadata": {
        "id": "ibgkZGdnY4SH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5da6400-009d-4739-962b-e91195833a18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 06:39:57--  https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp [following]\n",
            "--2024-03-24 06:39:57--  https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162403 (159K) [application/octet-stream]\n",
            "Saving to: ‚Äòsocial_media_data.csv‚Äô\n",
            "\n",
            "social_media_data.c 100%[===================>] 158.60K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-03-24 06:39:58 (52.4 MB/s) - ‚Äòsocial_media_data.csv‚Äô saved [162403/162403]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° notepad ‡∏´‡∏£‡∏∑‡∏≠ edittext ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ comma (,) ‡∏î‡∏±‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á\n",
        "\n",
        "![CSV file](https://mycourseville-default.s3.ap-southeast-1.amazonaws.com/useruploaded_course_files/2023_2/46110/materials/Screenshot_2567_03_09_at_14.00.36-465640-17099677538211.png)"
      ],
      "metadata": {
        "id": "C37ZR_29ceFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Colab (‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏ö‡πÄ‡∏ö‡∏¥‡πâ‡∏•‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå **`social_medial_data.csv`** ‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢) ‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô excel ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á\n",
        "\n",
        "![CSV file](https://mycourseville-default.s3.ap-southeast-1.amazonaws.com/useruploaded_course_files/2023_2/46110/materials/Screenshot_2567_03_09_at_13.59.46-465640-17099676757348.png)"
      ],
      "metadata": {
        "id": "hd5H3HcTcnhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "\n",
        "‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô 6 ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ö‡∏ô‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•‡∏°‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ (social_media_data.csv) ‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÜ\n",
        "\n",
        "* ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô 1-5 ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏•‡∏∞ 4 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô 6 ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 5 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô\n",
        "\n"
      ],
      "metadata": {
        "id": "IIjf4uXwvJ7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **`social_media_data(file_in)`**\n",
        "\n",
        "‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤\n",
        "* **`file_in`**   ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•‡∏°‡∏µ‡πÄ‡∏î‡∏µ‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô **`social_media_data.csv`**  ‡πÅ‡∏ï‡πà‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏™‡∏°‡∏≠ ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô\n",
        "\n",
        "‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **`Dict`** ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
        "* ‡∏Ñ‡πà‡∏≤ **`key`** ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Post_ID ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå social_media_data.csv\n",
        "* **`value`** ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Text), Platform, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retweet, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô likes, ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®, ‡πÅ‡∏•‡∏∞‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÇ‡∏û‡∏™‡∏ï‡πå ‡∏´‡∏£‡∏∑‡∏≠\n",
        " **`[Text, Platform, Retweets, Likes, Country, Year]`**\n",
        "‡πÇ‡∏î‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retweet ‡πÅ‡∏•‡∏∞ likes ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°\n",
        "\n",
        "* ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ **`strip()`** ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢\n",
        "* ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î UnicodeDecodeError ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Unicode (UTF-8) encoding ‡∏î‡∏±‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "```\n",
        "f = open(file_in, encoding=\"utf-8\")\n",
        "```\n",
        "\n",
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô\n",
        "* **`social_media_data(\"social_media_data.csv\")`**\n",
        "‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **`Dict`** ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\n",
        "```\n",
        "{'0000': ['Enjoying a beautiful day at the park!               #Nature #Park', 'Twitter', 19, 34, 'USA', '2023'],\n",
        "'0001': ['Traffic was terrible this morning.                  #Traffic #Morning', 'Twitter', 6, 18, 'Canada', '2023'],\n",
        "'0002': ['Just finished an amazing workout! üí™                #Fitness #Workout', 'Instagram', 21, 47, 'USA', '2023'],\n",
        "'0003': ['Excited about the upcoming weekend getaway!         #Travel #Adventure', 'Facebook', 9, 17, 'UK', '2023'],\n",
        "...,\n",
        "'0731':['Organizing a virtual talent show during challenging times bringing smiles to classmates' faces!  #VirtualEntertainment #HighSchoolPositivity\", 'Instagram', 24, 51, 'USA', '2020']}\n",
        "```\n",
        "\n",
        "* ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV (‡∏°‡∏µ‡πÅ‡∏ï‡πà‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á) ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **`Dict`** ‡∏ß‡πà‡∏≤‡∏á\n",
        "```\n",
        "{}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1zBDQiMDIskp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **`is_stopword(word)`**\n",
        "\n",
        "* ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏´‡∏£‡∏∑‡∏≠ **`word`**  ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô stopword ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÇ‡∏î‡∏¢‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô stopword ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠ stopwords.txt\n",
        "* Stop words ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß ‡πÜ ‡πÑ‡∏õ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢ ‡πÜ ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡∏´‡∏£‡∏∑‡∏≠ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ô‡∏±‡∏Å ‡πÄ‡∏ä‡πà‡∏ô a, an, the, also, just, quite, unless, etc.\n",
        "\n",
        "* ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ö‡∏π‡∏•‡∏•‡∏µ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏°‡∏∑‡πà‡∏≠ **`word`** ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô stopword ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πá‡∏à\n",
        "\n",
        "* ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏ ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏≥ stopword ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÄ‡∏ä‡πà‡∏ô can't, isn't, they'll ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≥‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô stopword ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡∏∞‡∏°‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏ä‡πà‡∏ô can't ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ cant, isn't ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ isnt, they'll ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏Å‡∏ß‡πà‡∏≤ theyll\n",
        "\n",
        "* ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô\n",
        "<table>\n",
        "<tr>\n",
        "  <td>word</td><td>is_stopword(word)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"Happy\"</td><td>False</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"birthDay\"</td><td>False</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"on\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"whiCH\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"Into\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"After\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"AFTER\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"afTEr\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"can't\"</td><td>False</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"cant\"</td><td>False</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "* ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î stopwords.txt file ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏µ‡πà‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ô‡∏µ‡πâ https://drive.google.com/file/d/1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh/view?usp=share_link"
      ],
      "metadata": {
        "id": "gP1GVJPpIi-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords.txt\n",
        "!wget https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh -O stopwords.txt"
      ],
      "metadata": {
        "id": "ozfbpG0MR5LU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555b8647-86a4-4573-99b0-8cb600d8a67c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 06:39:58--  https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh [following]\n",
            "--2024-03-24 06:39:58--  https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 761 [application/octet-stream]\n",
            "Saving to: ‚Äòstopwords.txt‚Äô\n",
            "\n",
            "stopwords.txt       100%[===================>]     761  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:39:59 (66.2 MB/s) - ‚Äòstopwords.txt‚Äô saved [761/761]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **`count_words_from_text(word_count_dict, text)`**\n",
        "\n",
        "‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠ **`text`** ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡πÉ‡∏ô  **`Dict`** ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ **`word_count_dict`** ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏ö‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Stop words\n",
        "\n",
        "‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤\n",
        "* **`word_count_dict`** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô **`Dict`** ‡πÄ‡∏Å‡πá‡∏ö key ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞ value ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
        "* **`text`** ‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡∏°‡∏≤‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô **`word_count_dict`**\n",
        "\n",
        "‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤\n",
        "* **`word_count_dict`** ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡πÉ‡∏ô **`text`**  ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢\n",
        "\n",
        "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
        "* ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô Enjoy ‡πÅ‡∏•‡∏∞ enjoy ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
        "* ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏´‡∏π‡∏û‡∏à‡∏ô‡πå‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏•‡∏∞‡∏Ñ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô pig ‡πÅ‡∏•‡∏∞ pigs ‡∏à‡∏∞‡∏°‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏•‡∏∞‡∏Ñ‡∏≥\n",
        "* ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ **`is_stopword(word)`** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Stop words ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÄ‡∏ä‡πà‡∏ô a ‡πÄ‡∏õ‡πá‡∏ô stop word <mark>‡πÑ‡∏°‡πà</mark>‡∏Ñ‡∏ß‡∏£‡∏ô‡∏≥‡∏°‡∏≤‡∏ô‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢\n",
        "* ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà .,:;?()[]\"'{}-/\\|_!\n",
        "* ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î Hashtag (#) ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏•‡∏∞‡∏Ñ‡∏≥‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î # ‡πÄ‡∏ä‡πà‡∏ô #Enjoy ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ô‡∏•‡∏∞‡∏Ñ‡∏≥‡∏Å‡∏±‡∏ö Enjoy (‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ # ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß ‡πÜ)\n",
        "* ‡∏´‡∏≤‡∏Å‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏Ñ‡∏±‡πà‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏á‡πà‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å ‡πÄ‡∏ä‡πà‡∏ô soul-stirring ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ß‡πà‡∏≤ soulstirring, Jay-Z ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ jayz, Can't ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ cant\n",
        "* ‡∏•‡∏≥‡∏î‡∏±‡∏ö key ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏à‡∏≤‡∏Å **`word_count_dict`**  ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡πà‡∏≤\n",
        "\n",
        "\n",
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>word_count_dict</td>\n",
        "  <td>text</td>\n",
        "  <td>count_words_from_text(word_count_dict, text) -> ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 1, 'market': 1}</td>\n",
        "  <td>\"Hello today! #Happy\"</td>\n",
        "  <td>{'aroma': 1, 'market': 1, 'hello': 1, 'today': 1, '#happy': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 1, 'market': 1}</td>\n",
        "  <td>\"Exploring the local market today, can't wait to share with everyone!\"</td>\n",
        "  <td>{'aroma': 1, 'market': 2, 'exploring': 1, 'local': 1, 'today': 1, 'cant': 1, 'wait': 1, 'share': 1, 'everyone': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 1, 'market': 1}</td>\n",
        "  <td>\"LoVe Aroma THeRapy!! LoVed it LoVe it #AROMA.\"</td>\n",
        "  <td>{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{}</td>\n",
        "  <td>\"Mary has \"A little Lamb\", LiTTLE Lamb, MARY has a Little cat!! #lamb #cat\"</td>\n",
        "  <td>{'mary': 2, 'little': 3, 'lamb': 2, 'cat': 1, '#lamb': 1, '#cat': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{}</td>\n",
        "  <td>\"Mary has A lot of Lambs, a little lamb.\"</td>\n",
        "  <td>{'mary': 1, 'lot': 1, 'lambs': 1, 'little': 1, 'lamb': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'mary': 2, 'little': 3}</td>\n",
        "  <td> \"Twinkle twinkle 'little' star..... How I wonder what you are.\"</td>\n",
        "  <td>{'mary': 2, 'little': 4, 'twinkle': 2, 'star': 1, 'wonder': 1}</td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "AnGiqJ9iI1Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  **`count_words_from_data_dict(data_dict, year, country, platform)`**\n",
        "\n",
        "‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å\n",
        "* **`data_dict`** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö **`Dict`** ‡∏ó‡∏µ‡πà‡∏°‡∏µ **`key`**  ‡πÄ‡∏õ‡πá‡∏ô **`Post_ID`** ‡πÅ‡∏•‡∏∞ **`value`** ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Text), Platform, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retweet, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô likes, ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®, ‡πÅ‡∏•‡∏∞‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÇ‡∏û‡∏™‡∏ï‡πå ‡∏´‡∏£‡∏∑‡∏≠\n",
        " **`[Text, Platform, Retweets, Likes, country, Year]`**\n",
        "‡πÇ‡∏î‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retweet ‡πÅ‡∏•‡∏∞ likes ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°\n",
        "* **`year`** ‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÉ‡∏î‡∏õ‡∏µ‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô 2023 ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ 'all' ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å ‡πÜ ‡∏õ‡∏µ\n",
        "* **`country`** ‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô USA ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ 'all' ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å ‡πÜ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®\n",
        "* **`platform`** ‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÉ‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô Facebook ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ 'all' ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å ‡πÜ ‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°\n",
        "\n",
        "‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **`Dict`** ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏õ‡∏µ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡πÅ‡∏•‡∏∞‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "\n",
        "‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
        "* ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ **`count_words_from_text(word_count_dict, text)`**\n",
        "* ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö year, country, ‡πÅ‡∏•‡∏∞ platform ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå <mark>J</mark>apan ‡πÄ‡∏õ‡πá‡∏ô <mark>j</mark>apan ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡πÅ‡∏ï‡πà <mark>J</mark>apan)\n",
        "\n",
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô\n",
        "* **`data_dict`**  ‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô\n",
        "  * **`data_dict = social_media_data(\"social_media_data.csv\")`** ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô\n",
        "  ```\n",
        "{'0000': ['enjoying a beautiful day at the park!               #nature #park', 'Twitter', 19, 34, 'USA', '2023'],\n",
        "'0001': ['traffic was terrible this morning.                  #traffic #morning', 'Twitter', 6, 18, 'Canada', '2023'],\n",
        "'0002': ['just finished an amazing workout! üí™                #fitness #workout', 'Instagram', 21, 47, 'USA', '2023'],\n",
        "'0003': ['excited about the upcoming weekend getaway!         #travel #adventure', 'Facebook', 9, 17, 'UK', '2023'],\n",
        "...,\n",
        "'0731':['\"organizing a virtual talent show during challenging times bringing smiles to classmates' faces!  #virtualentertainment #highschoolpositivity\", 'Instagram', 24, 51, 'USA', '2020']}\n",
        "```\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>year</td><td>country</td><td>platform</td><td>count_words_from_data_dict(data_dict, year, country, platform)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'all'</td><td>'all'</td><td>'all'</td><td>{'enjoying': 5, 'beautiful': 4, 'day': 26, 'park': 3, ... } (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 3446 ‡∏Ñ‡∏≥) </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2023'</td><td>'USA'</td><td>'Facebook'</td><td>{'reflecting': 1, 'past': 1, 'looking': 1, 'ahead': 1, '#reflection': 1, ... } (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 313 ‡∏Ñ‡∏≥) </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'all'</td><td>'Thailand'</td><td>'all'</td><td>{'heart': 1, 'bustling': 1, 'market': 1, 'street': 1, 'food': 1, 'connoisseur': 1, 'indulges': 1, 'culinary': 1, 'adventure': 1, 'savoring': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2023'</td><td>'Thailand'</td><td>'all'</td><td>{}\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2020'</td><td>'Japan'</td><td>'Twitter'</td><td>{'avoiding': 1, 'shards': 1, 'shattered': 1, 'dreams': 1, 'walking': 1, 'tightrope': 1, 'resilience': 1, '#resilience': 1, '#tightropewalk': 1}\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2020'</td><td>'<mark>j</mark>apan'</td><td>'<mark>t</mark>witter'</td><td>{}\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "GSNkn5JMJEhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **`top_k_words(word_count_dict, k)`**\n",
        "‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î k ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô <mark>‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ </mark>\n",
        "\n",
        "‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤\n",
        "* **`word_count_dict`** ‡πÄ‡∏õ‡πá‡∏ô **`Dict`** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥\n",
        "* **`k`** ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î k ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö\n",
        "\n",
        "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤\n",
        "* ‡∏Ñ‡∏∑‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏≤‡∏°‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°\n",
        "  * ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏°‡∏µ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ k ‡∏Ñ‡πà‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ 3 ‡∏Ñ‡∏≥‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÉ‡∏ô **`Dict`**  ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡∏°‡∏µ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 2 ‡∏ñ‡∏∂‡∏á 4 ‡∏Ñ‡∏≥ ‡∏Å‡πá‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 2 ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "```\n",
        " {'dreams':15,'like':10,'feeling':10,'journey':10,'sky':10,'challenges':9,'day':9,'new':9,'world':9}\n",
        " ```\n",
        "    * ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô **`['dreams', 'feeling', 'journey', 'like', 'sky']`** ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 2 ‡∏ã‡πâ‡∏≥ 4 ‡∏Ñ‡∏≥ ‡∏à‡∏∂‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏•‡∏¢\n",
        "  * ‡∏´‡∏≤‡∏Å‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÉ‡∏´‡∏Ñ‡∏∑‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ñ‡∏≥‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°\n",
        "  \n",
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô\n",
        "<table>\n",
        "<tr>\n",
        "  <td>word_count_dict</td>\n",
        "  <td>k</td>\n",
        "  <td>top_k_words(word_count_dict, k)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22}</td><td>5</td>\n",
        "  <td>['new', 'like', 'day', 'feeling', 'dreams']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22}</td><td>3</td>\n",
        "  <td>['new', 'like', 'day', 'feeling']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}<td>5</td>\n",
        "  <td>['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}<td>3</td>\n",
        "  <td>['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'market': 1, 'aroma': 1}<td>5</td>\n",
        "  <td>['aroma', 'market']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'market': 1, 'aroma': 1}<td>3</td>\n",
        "  <td>['aroma', 'market']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}<td>5</td>\n",
        "  <td>['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}<td>3</td>\n",
        "  <td>['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']</td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "4xZOmztQI75Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  **count_word_summary(file_in, file_out, k, year, country, platform)**\n",
        "\n",
        "‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î k ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ‡πÉ‡∏ô ‡∏õ‡∏µ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡πÅ‡∏•‡∏∞‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "\n",
        "‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤\n",
        "* ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå **`file_in`** ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÑ‡∏ü‡∏•‡πå **`social_medial_data.csv`**\n",
        "* **`file_out`** ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå output (‡πÄ‡∏õ‡πá‡∏ô .txt) ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏™‡∏∏‡∏î ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î **`k`** ‡∏Ñ‡∏≥ ‡∏î‡∏π‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡∏î‡∏∏‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö **`k`**‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô 5.)\n",
        "* **`year`** ‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÉ‡∏î‡∏õ‡∏µ‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô 2023 ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ 'all' ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å ‡πÜ ‡∏õ‡∏µ\n",
        "* **`country`** ‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô USA ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ 'all' ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å ‡πÜ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®\n",
        "* **`platform`** ‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÉ‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô Facebook ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ 'all' ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å ‡πÜ ‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°\n",
        "\n",
        "<mark>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤</mark>\n",
        "\n",
        "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
        "* ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô 1-5 ‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡∏ô‡∏≥‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏£‡∏∏‡∏õ\n",
        "* ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö year, country, ‡πÅ‡∏•‡∏∞ platform ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå <mark>J</mark>apan ‡πÄ‡∏õ‡πá‡∏ô <mark>j</mark>apan ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå **`file_out`** ‡∏ß‡πà‡∏≤ No data(‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡πÅ‡∏ï‡πà <mark>J</mark>apan)\n",
        "\n",
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>file_in</td><td>file_out</td><td>k</td><td>year</td><td>country</td><td>platform</td><td>‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô file_out</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'all'</td><td>'all'</td><td>'all'</td>\n",
        "  <td>new:43</br>like:27</br>day:26</br>feeling:26</br>dreams:25</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'all'</td><td>'all'</td><td>'all'</td>\n",
        "  <td>new:43</br>like:27</br>day:26</br>feeling:26</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'2023'</td><td>'USA'</td><td>'Facebook'</td>\n",
        "  <td>art:4</br>every:3</br>experience:3</br>exploring:3</br>local:3</br>love:3</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'2023'</td><td>'USA'</td><td>'Facebook'</td>\n",
        "  <td>art:4</br>every:3</br>experience:3</br>exploring:3</br>local:3</br>love:3</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'all'</td><td>'Thailand'</td><td>'all'</td>\n",
        "  <td>#culinaryadventure:1</br>#streetfooddelights:1</br>adventure:1</br>aromas:1</br>bustling:1</br>connoisseur:1</br>culinary:1</br>diverse:1\n",
        "  </br>flavors:1\n",
        "  </br>food:1\n",
        "  </br>heart:1\n",
        "  </br>indulges:1\n",
        "  </br>market:1\n",
        "  </br>savoring:1\n",
        "  </br>street:1\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'2023'</td><td>'Thailand'</td><td>'all'</td>\n",
        "  <td>No data\n",
        "  </td>\n",
        "  </tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'2023'</td><td>'UK'</td><td>'Instagram'</td>\n",
        "  <td>new:7</br>adventure:4</br>friends:4</br>art:3</br>day:3</br>magic:3</br>weekend:3</br>wine:3\n",
        "  </br>years:3\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'all'</td><td>'India'</td><td>'all'</td>\n",
        "  <td>day:5</br>new:5</br>painting:5</br>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'all'</td><td>'India'</td><td>'all'</td>\n",
        "  <td>day:5</br>new:5</br>painting:5\n",
        "  </br>#gratitude:4\n",
        "  </br>#hopeful:4\n",
        "  </br>dreams:4\n",
        "  </br>hopeful:4\n",
        "  </br>life:4\n",
        "  </br>optimism:4\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "393bjIKfJI79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HW4_Social_Media_Data\n",
        "\n",
        "# Input CSV file and return all data in CSV file as a dict\n",
        "# Key: Post_ID\n",
        "# Value: [Text, Platform, Retweets, Likes, Country, Year]\n",
        "# From social_media_data.csv header, we have\n",
        "# Post_ID,Text,Sentiment,Timestamp,User,Platform,Retweets,Likes,Country,Year,Month,Day,Hour\n",
        "# Since the first line of the CSV file is the header, we need to skip it.\n",
        "# After skip it, we need to split data with comma (,) and input an a data in index 1,5,6,7,8,9\n",
        "# Don't forget to convert Retweets and Likes to integer.\n",
        "def social_media_data(file_in):\n",
        "    f = open(file_in, encoding=\"utf-8\")\n",
        "    social_dict = {}\n",
        "    IsFirstLine = True\n",
        "    for line in f:\n",
        "        # Skip first line (Header)\n",
        "        if(not IsFirstLine):\n",
        "            data = line.strip().split(',')\n",
        "            social_dict[data[0]] = [data[1].strip(), data[5].strip(),\n",
        "                                    int(data[6]), int(data[7]),\n",
        "                                    data[8].strip(), data[9].strip()]\n",
        "        else:\n",
        "            IsFirstLine = False\n",
        "    return social_dict\n",
        "\n",
        "# Check if a parameter 'word' is in the file stopwords.txt\n",
        "# Consider lowercase and uppercase as a same character\n",
        "def is_stopword(word):\n",
        "    f = open(\"stopwords.txt\", \"r\")\n",
        "    # Get a list of stopwords from stopwords.txt\n",
        "    stopwords = f.readline().strip().split(\", \")\n",
        "    for i in range(len(stopwords)):\n",
        "        stopwords[i] = stopwords[i].strip()\n",
        "    # Check if the word is stopword\n",
        "    if(word.lower() in stopwords):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Count a word (except stopwords) in text and update them in word_count_dict\n",
        "# - Consider lowercase and uppercase as a same character\n",
        "# - Consider singular and plural word as different word\n",
        "# - Consider hashtag word as different word (Example: #Enjoy and enjoy is not the same)\n",
        "#   and guaruntee that there are no single '#' in a text\n",
        "# - Consider a word like \"can't\" --> cant, \"soul-stirring\" --> \"soulstirring\" and \"Jay-Z\" --> \"jayz\"\n",
        "# - Beware of \".,:;?()[]\"'{}-/|_!\"\n",
        "# - No need to order keys in word_count_dict\n",
        "def count_words_from_text(word_count_dict, text):\n",
        "    # Split a string into a list of words\n",
        "    words = text.split()\n",
        "    # Convert all words in a list into lowercase and remove all symbols\n",
        "    for i in range(len(words)):\n",
        "        words[i] = words[i].lower()\n",
        "        temp = \"\"\n",
        "        for char in words[i]:\n",
        "            if(char not in \".,:;?()[]\\\"\\'{}-/|_!\"):\n",
        "                temp += char\n",
        "        words[i] = temp\n",
        "    # Count words\n",
        "    for word in words:\n",
        "        if(not is_stopword(word)):\n",
        "            if(word not in word_count_dict):\n",
        "                word_count_dict[word] = 1\n",
        "            else:\n",
        "                word_count_dict[word] += 1\n",
        "    return word_count_dict\n",
        "\n",
        "# This function filter a data by year, country and platform and returns as a word count dict\n",
        "# Parameter 'data_dict' keys is 'Post_ID' and value is [Text, Platform, Retweets, Likes, Country, Year]\n",
        "# Count words in 'Text' in each value in data_dict\n",
        "# If year = 'all', output data in all year from a data_dict\n",
        "# If country = 'all', output data in all country from data_dict\n",
        "# If platform = 'all', output data in all platform from\n",
        "# Uppercase and lowercase are not the same\n",
        "def count_words_from_data_dict(data_dict, year, country, platform):\n",
        "    words_count = {}\n",
        "    for key in data_dict:\n",
        "        # All year, country and platform\n",
        "        if(year == 'all' and country == 'all' and platform == 'all'):\n",
        "            words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All year and country. Filter only platform\n",
        "        elif(year == 'all' and country == 'all'):\n",
        "            if(data_dict[key][1] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All year and platform. Filter only country\n",
        "        elif(year == 'all' and platform == 'all'):\n",
        "            if(data_dict[key][4] == country):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All country and platform. Filter only year\n",
        "        elif(country == 'all' and platform == 'all'):\n",
        "            if(data_dict[key][5] == year):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All year. Filter country and platform\n",
        "        elif(year == 'all'):\n",
        "            if(data_dict[key][4] == country and data_dict[key][5] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All country. Filter year and platform\n",
        "        elif(country == 'all'):\n",
        "            if(data_dict[key][5] == year and data_dict[key][1] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All platform. Filter year and country\n",
        "        elif(platform == 'all'):\n",
        "            if(data_dict[key][5] == year and data_dict[key][4] == country):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # Filter year, country and platform\n",
        "        else:\n",
        "            if(data_dict[key][5] == year and data_dict[key][4] == country and data_dict[key][1] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "    return words_count\n",
        "\n",
        "# This function returns top 'k' highest word count. Returns as a list with words\n",
        "# If the words have the same count, then sort them in alphabetical order\n",
        "# In case of k is greater than len(unique_count), returns all words but in order\n",
        "# In case of length of list 'words' is already greater than k, break the loop\n",
        "def top_k_words(word_count_dict, k):\n",
        "    # Find unique_count from word_count_dict\n",
        "    # Example: word_count_dict = {'dreams':15,,'day':9,'like':10,'feeling':10,'challenges':9}\n",
        "    # unique_count = [15,10,9]\n",
        "    unique_count = []\n",
        "    for key in word_count_dict:\n",
        "        if(word_count_dict[key] not in unique_count):\n",
        "            unique_count.append(word_count_dict[key])\n",
        "    unique_count.sort(reverse = True)\n",
        "    # Find and return a list of top 'k' highest word count\n",
        "    words = []\n",
        "    # Use min(k, len(unique_count) in case of k is greater than len(unique_count)\n",
        "    for i in range(min(k, len(unique_count))):\n",
        "        temp = []\n",
        "        if(len(words) < k):\n",
        "            for key in word_count_dict:\n",
        "                if(word_count_dict[key] == unique_count[i]):\n",
        "                    temp.append(key)\n",
        "            temp.sort()\n",
        "            for item in temp:\n",
        "                words.append(item)\n",
        "        # Break the loop in case of length of list 'words' is already greater than k\n",
        "        else:\n",
        "            break\n",
        "    return words\n",
        "\n",
        "# Use all previous function to solve this\n",
        "# - file_in = social_media_data.csv\n",
        "# - file_out = top 'k' word count (.txt)\n",
        "# - year = if 'all', then consider all year\n",
        "# - country = if 'all', then consider all country\n",
        "# - platform = if 'all', then consider all platform\n",
        "# - This function does not return anything but edit/create file in parameter 'file_out'\n",
        "# - If there is no data, then edit text in 'file_out' as 'No data'\n",
        "def count_word_summary(file_in, file_out, k, year, country, platform):\n",
        "    social_dict = social_media_data(file_in)\n",
        "    count_words = count_words_from_data_dict(social_dict, year, country, platform)\n",
        "    top_k = top_k_words(count_words, k)\n",
        "    f = open(file_out, 'w')\n",
        "    text = \"\"\n",
        "    for word in top_k:\n",
        "        text += word + \":\" + str(count_words[word]) + \"\\n\"\n",
        "    if(text == \"\"):\n",
        "        f.write(\"No data\")\n",
        "    else:\n",
        "        f.write(text)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "32TQlQREdOVp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö**\n",
        "\n",
        "1. ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà run ‡πÅ‡∏•‡πâ‡∏ß‡∏ú‡πà‡∏≤‡∏ô ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° `... ok`\n",
        "\n",
        "‡πÄ‡∏ä‡πà‡∏ô\n",
        "```\n",
        "test_1_social_media_data (__main__.TestHW4) ... ok\n",
        "test_2_is_stopword (__main__.TestHW4) ... ok\n",
        "test_3_count_words_from_text (__main__.TestHW4) ... ok\n",
        "test_4_count_words_from_data_dict (__main__.TestHW4) ... ok\n",
        "test_5_top_k_words (__main__.TestHW4) ... ok\n",
        "test_6_count_word_summary (__main__.TestHW4) ... ok\n",
        "```\n",
        "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á 6 ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
        "\n",
        "\n",
        "2. ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° `... skipped` ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô\n",
        "\n",
        "```\n",
        "test_3_count_words_from_text (__main__.TestHW4) ... skipped '\"count_words_from_text()\" is not defined or not implemented'\n",
        "```\n",
        "\n",
        "‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà 3 ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô count_words_from_text()\n",
        "\n",
        "\n",
        "\n",
        "3. ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡πà‡∏ô‡πÉ‡∏î<mark>‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á</mark> run ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ\n",
        "‡πÄ‡∏Ñ‡∏™‡∏ô‡∏±‡πâ‡∏ô ‡πÜ ‡∏à‡∏∞<mark>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ok ‡πÅ‡∏•‡∏∞ skipped</mark>\n",
        "```\n",
        "test_6_count_word_summary (__main__.TestHW4) ...\n",
        "```\n",
        "‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\n",
        "\n",
        "```\n",
        "======================================================================\n",
        "FAIL: test_6_count_word_summary (__main__.TestHW4) (i=1)\n",
        "----------------------------------------------------------------------\n",
        "Traceback (most recent call last):\n",
        "  File \"/hw4_testcases.py\", line 205, in test_6_count_word_summary\n",
        "    self.assertEqual(\"\".join(f.readlines()).strip(), test_case['expected'])\n",
        "AssertionError: 'new:43\\nlike:27\\nday:26\\nfeeling:26' != 'new:43\\nlike:27\\nday:26\\nfeeling:27'\n",
        "  new:43\n",
        "  like:27\n",
        "  day:26\n",
        "- feeling:26?          ^\n",
        "+ feeling:27?          ^\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡πÉ‡∏ô `test_6_count_word_summary` .‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏¢‡πà‡∏≠‡∏¢‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 2 (‡∏î‡∏π‡∏ï‡∏£‡∏á i=1 ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà i = 0)\n"
      ],
      "metadata": {
        "id": "Ofeef8wy_qxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download files ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô"
      ],
      "metadata": {
        "id": "4sFUo5g9AXhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp -O social_media_data.csv\n",
        "!wget https://drive.google.com/uc?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX -O empty_data.csv\n",
        "!wget https://drive.google.com/uc?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW -O 4_1.txt\n",
        "!wget https://drive.google.com/uc?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y -O 4_2.txt\n",
        "!wget https://drive.google.com/uc?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz -O 1_1.txt\n",
        "!wget https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh -O stopwords.txt"
      ],
      "metadata": {
        "id": "Sv1oFeHlAa2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef30a2e-e907-4e2c-ca0e-849865eb6612"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 06:39:59--  https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp [following]\n",
            "--2024-03-24 06:39:59--  https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162403 (159K) [application/octet-stream]\n",
            "Saving to: ‚Äòsocial_media_data.csv‚Äô\n",
            "\n",
            "social_media_data.c 100%[===================>] 158.60K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-03-24 06:39:59 (111 MB/s) - ‚Äòsocial_media_data.csv‚Äô saved [162403/162403]\n",
            "\n",
            "--2024-03-24 06:39:59--  https://drive.google.com/uc?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX [following]\n",
            "--2024-03-24 06:39:59--  https://drive.usercontent.google.com/download?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89 [application/octet-stream]\n",
            "Saving to: ‚Äòempty_data.csv‚Äô\n",
            "\n",
            "empty_data.csv      100%[===================>]      89  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:39:59 (2.06 MB/s) - ‚Äòempty_data.csv‚Äô saved [89/89]\n",
            "\n",
            "--2024-03-24 06:39:59--  https://drive.google.com/uc?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW [following]\n",
            "--2024-03-24 06:39:59--  https://drive.usercontent.google.com/download?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62951 (61K) [application/octet-stream]\n",
            "Saving to: ‚Äò4_1.txt‚Äô\n",
            "\n",
            "4_1.txt             100%[===================>]  61.48K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-03-24 06:40:00 (73.4 MB/s) - ‚Äò4_1.txt‚Äô saved [62951/62951]\n",
            "\n",
            "--2024-03-24 06:40:00--  https://drive.google.com/uc?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y [following]\n",
            "--2024-03-24 06:40:00--  https://drive.usercontent.google.com/download?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5497 (5.4K) [application/octet-stream]\n",
            "Saving to: ‚Äò4_2.txt‚Äô\n",
            "\n",
            "4_2.txt             100%[===================>]   5.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:40:00 (71.1 MB/s) - ‚Äò4_2.txt‚Äô saved [5497/5497]\n",
            "\n",
            "--2024-03-24 06:40:01--  https://drive.google.com/uc?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz [following]\n",
            "--2024-03-24 06:40:01--  https://drive.usercontent.google.com/download?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 122118 (119K) [application/octet-stream]\n",
            "Saving to: ‚Äò1_1.txt‚Äô\n",
            "\n",
            "1_1.txt             100%[===================>] 119.26K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-03-24 06:40:01 (64.9 MB/s) - ‚Äò1_1.txt‚Äô saved [122118/122118]\n",
            "\n",
            "--2024-03-24 06:40:01--  https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh [following]\n",
            "--2024-03-24 06:40:01--  https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 761 [application/octet-stream]\n",
            "Saving to: ‚Äòstopwords.txt‚Äô\n",
            "\n",
            "stopwords.txt       100%[===================>]     761  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:40:02 (62.8 MB/s) - ‚Äòstopwords.txt‚Äô saved [761/761]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "def is_function_defined(func_name):\n",
        "    return func_name in globals()\n",
        "\n",
        "def skip_if_not_implemented(func_name):\n",
        "    def decorator(test_func):\n",
        "        if not is_function_defined(func_name):\n",
        "            return unittest.skip(\"\\\"\" + func_name + \"()\\\" is not defined or not implemented\")(test_func)\n",
        "        return test_func\n",
        "    return decorator\n",
        "\n",
        "\n",
        "class TestHW4(unittest.TestCase):\n",
        "\n",
        "    @skip_if_not_implemented('social_media_data')\n",
        "    def test_1_social_media_data(self):\n",
        "        test_case = {'args':'social_media_data.csv', 'expected':'1_1.txt'}\n",
        "        result = social_media_data(test_case['args'])\n",
        "        fsol = open(test_case['expected'],'r',encoding=\"utf-8\")\n",
        "        self.assertEqual(str(sorted(result.items())), \"\".join(fsol.readlines()))\n",
        "        fsol.close()\n",
        "\n",
        "        test_case = {'args':'empty_data.csv', 'expected': {}}\n",
        "        result = social_media_data(test_case['args'])\n",
        "        self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "\n",
        "    @skip_if_not_implemented('is_stopword')\n",
        "    def test_2_is_stopword(self):\n",
        "        test_cases = [\n",
        "            {'args':\"Happy\", 'expected':False},\n",
        "            {'args':\"birthDay\", 'expected':False},\n",
        "            {'args':\"on\", 'expected':True},\n",
        "            {'args':\"whiCH\", 'expected':True},\n",
        "            {'args':\"Into\", 'expected':True},\n",
        "            {'args':\"After\", 'expected':True},\n",
        "            {'args':\"AFTER\", 'expected':True},\n",
        "            {'args':\"afTEr\", 'expected':True},\n",
        "            {'args':\"can't\", 'expected':False},\n",
        "            {'args':\"cant\", 'expected':False}\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                result = is_stopword(test_case['args'])\n",
        "                self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('count_words_from_text')\n",
        "    def test_3_count_words_from_text(self):\n",
        "        test_cases = [\n",
        "            {'args':[{'aroma': 1, 'market': 1},\"Hello today! #Happy\"], 'expected':{'aroma': 1, 'market': 1, 'hello': 1, 'today': 1, '#happy': 1}},\n",
        "            {'args':[{'aroma': 1, 'market': 1},\"Exploring the local market today, can't wait to share with everyone!\"], 'expected':{'aroma': 1, 'market': 2, 'exploring': 1, 'local': 1, 'today': 1, 'cant': 1, 'wait': 1, 'share': 1, 'everyone': 1}},\n",
        "            {'args':[{'aroma': 1, 'market': 1},\"LoVe Aroma THeRapy!! LoVed it LoVe it #AROMA.\"], 'expected':{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}},\n",
        "            {'args':[{},\"Mary has \\\"A little Lamb\\\", LiTTLE Lamb, MARY has a Little cat!! #lamb #cat\"], 'expected':{'mary': 2, 'little': 3, 'lamb': 2, 'cat': 1, '#lamb': 1, '#cat': 1}},\n",
        "            {'args':[{},\"Mary has A lot of Lambs, a little lamb.\"], 'expected':{'mary': 1, 'lot': 1, 'lambs': 1, 'little': 1, 'lamb': 1}},\n",
        "            {'args':[{'mary': 2, 'little': 3},\"Twinkle twinkle 'little' star..... How I wonder what you are.\"], 'expected':{'mary': 2, 'little': 4, 'twinkle': 2, 'star': 1, 'wonder': 1}},\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                d,t = test_case['args']\n",
        "                result = count_words_from_text(d,t)\n",
        "                self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('count_words_from_data_dict')\n",
        "    def test_4_count_words_from_data_dict(self):\n",
        "        test_cases = [\n",
        "            {'args':['all','all','all'], 'expected':'4_1.txt'},\n",
        "            {'args':['2023','USA','Facebook'], 'expected':'4_2.txt'},\n",
        "            {'args':['all','Thailand','all'], 'expected':{'heart': 1, 'bustling': 1, 'market': 1, 'street': 1, 'food': 1, 'connoisseur': 1, 'indulges': 1, 'culinary': 1, 'adventure': 1, 'savoring': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}},\n",
        "            {'args':['2023','Thailand','all'], 'expected':{}},\n",
        "            {'args':['2020','Japan','Twitter'], 'expected':{'avoiding': 1, 'shards': 1, 'shattered': 1, 'dreams': 1, 'walking': 1, 'tightrope': 1, 'resilience': 1, '#resilience': 1, '#tightropewalk': 1}},\n",
        "            {'args':['2020','japan','twitter'], 'expected':{}},\n",
        "            ]\n",
        "        data_dict = social_media_data(\"social_media_data.csv\")\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                y,c,p = test_case['args']\n",
        "                result = count_words_from_data_dict(data_dict,y,c,p)\n",
        "                if i < 2:\n",
        "                    fsol = open(test_case['expected'],'r',encoding=\"utf-8\")\n",
        "                    self.assertEqual(str(sorted(result.items())), \"\".join(fsol.readlines()))\n",
        "                    fsol.close()\n",
        "                else:\n",
        "                    self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('top_k_words')\n",
        "    def test_5_top_k_words(self):\n",
        "        test_cases = [\n",
        "            {'args':[{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22},5], 'expected':['new', 'like', 'day', 'feeling', 'dreams']},\n",
        "            {'args':[{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22},3], 'expected':['new', 'like', 'day', 'feeling']},\n",
        "            {'args':[{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1},5], 'expected':['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']},\n",
        "            {'args':[{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1},3], 'expected':['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']},\n",
        "            {'args':[{'market': 1, 'aroma': 1},5], 'expected':['aroma', 'market']},\n",
        "            {'args':[{'market': 1, 'aroma': 1},3], 'expected':['aroma', 'market']},\n",
        "            {'args':[{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1},5], 'expected':['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']},\n",
        "            {'args':[{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1},3], 'expected':['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']},\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                d,k = test_case['args']\n",
        "                result = top_k_words(d,k)\n",
        "                self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('count_word_summary')\n",
        "    def test_6_count_word_summary(self):\n",
        "        test_cases = [\n",
        "            {'args':['social_media_data.csv','summary.txt',5,'all','all','all'], 'expected':'new:43\\nlike:27\\nday:26\\nfeeling:26\\ndreams:25'},\n",
        "            {'args':['social_media_data.csv','summary2.txt',3,'all','all','all'], 'expected':'new:43\\nlike:27\\nday:26\\nfeeling:26'},\n",
        "            {'args':['social_media_data.csv','summary.txt',5,'2023','USA','Facebook'], 'expected':'art:4\\nevery:3\\nexperience:3\\nexploring:3\\nlocal:3\\nlove:3'},\n",
        "            {'args':['social_media_data.csv','summary2.txt',3,'2023','USA','Facebook'], 'expected':'art:4\\nevery:3\\nexperience:3\\nexploring:3\\nlocal:3\\nlove:3'},\n",
        "            {'args':['social_media_data.csv','summary.txt',5,'all','Thailand','all'], 'expected':'#culinaryadventure:1\\n#streetfooddelights:1\\nadventure:1\\naromas:1\\nbustling:1\\nconnoisseur:1\\nculinary:1\\ndiverse:1\\nflavors:1\\nfood:1\\nheart:1\\nindulges:1\\nmarket:1\\nsavoring:1\\nstreet:1'},\n",
        "            {'args':['social_media_data.csv','summary1.txt',3,'2023','Thailand','all'], 'expected':'No data'},\n",
        "            {'args':['social_media_data.csv','summary2.txt',5,'2023','UK','Instagram'], 'expected':'new:7\\nadventure:4\\nfriends:4\\nart:3\\nday:3\\nmagic:3\\nweekend:3\\nwine:3\\nyears:3'},\n",
        "            {'args':['social_media_data.csv','summary3.txt',3,'all','India','all'], 'expected':'day:5\\nnew:5\\npainting:5'},\n",
        "            {'args':['social_media_data.csv','summary4.txt',5,'all','India','all'], 'expected':'day:5\\nnew:5\\npainting:5\\n#gratitude:4\\n#hopeful:4\\ndreams:4\\nhopeful:4\\nlife:4\\noptimism:4'},\n",
        "\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                file_in, file_out, k, year, country, platform = test_case['args']\n",
        "                count_word_summary(file_in, file_out, k, year, country, platform)\n",
        "                f = open(file_out,'r',encoding=\"utf-8\")\n",
        "                self.assertEqual(\"\".join(f.readlines()).strip(), test_case['expected'])\n",
        "                f.close()\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtlurKcT_tfA",
        "outputId": "99dfaac7-b5b6-4048-d114-fac3f3726af0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_1_social_media_data (__main__.TestHW4) ... ok\n",
            "test_2_is_stopword (__main__.TestHW4) ... ok\n",
            "test_3_count_words_from_text (__main__.TestHW4) ... ok\n",
            "test_4_count_words_from_data_dict (__main__.TestHW4) ... ok\n",
            "test_5_top_k_words (__main__.TestHW4) ... ok\n",
            "test_6_count_word_summary (__main__.TestHW4) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 1.248s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7c74c800b100>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}