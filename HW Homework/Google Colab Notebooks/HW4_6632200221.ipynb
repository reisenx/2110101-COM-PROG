{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# การบ้านครั้งที่ 4: Social Media Data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YTDQanrJ86Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**จุดมุ่งหมายของการบ้าน**\n",
        "- เข้าใจและสามารถใช้งาน\n",
        "  - String and file processing\n",
        "  - Basic dict\n"
      ],
      "metadata": {
        "id": "mXvbUjnTOV2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##**เริ่มต้น**\n",
        "1. ให้ copy file ไปที่ Google Drive ของนิสิตโดยเลือกเมนู File->Save a copy in Drive\n",
        "2. ไปที่ไฟล์ที่ copy มาแล้วตั้งชื่อไฟล์เป็น HW4_XXXXXXXXXX.ipynb เมื่อ XXXXXXXXXX คือเลขประจำตัวนิสิต\n",
        "\n",
        "## <font color=red>สำคัญ: อ่านตรงนี้ด้วย\n",
        "## ข้อห้าม</font>\n",
        " - ห้าม import ใด ๆ\n",
        " - ฟังก์ชันต้องไม่ print ใด ๆ นอกเหนือจากคำสั่ง\n",
        " - ห้ามเปลี่ยนบรรทัด `def` ของฟังก์ชันที่ให้เขียน\n",
        " - ในกรณีที่ฟังก์ชันมีการคืนค่า ฟังก์ชันต้องคืนข้อมูลและประเภทข้อมูลตามที่กำหนดเท่านั้น\n",
        " - ฟังก์ชันต้องไม่เปลี่ยนแปลงข้อมูลภายในของพารามิเตอร์ที่ได้รับ\n",
        " - บรรทัดแรกของ code cell ที่เขียนต้องขึ้นต้นด้วย `# HW4_Social_Media_Data` ตามที่ให้ไป ห้ามแก้ไขหรือเพิ่มอะไรเข้าไปก่อนหน้า\n",
        " - สามารถเขียนฟังก์ชันตัวเองได้ในพื้นที่ด้านล่างสุดของเซลล์ที่บอกว่า\n",
        " ```\n",
        " # Write your functions here ONLY (If any)\n",
        " ```\n",
        "\n",
        "- <font color=red>ส่อทุจริต</font> เช่น  \n",
        " - ส่งโปรแกรมที่ผู้ส่งไม่สามารถอธิบายได้ว่า ใช้หลักการและทำงานอย่างไร\n",
        "  - หรือ ส่งโปรแกรมที่คล้ายกับโปรแกรมของผู้อื่นมาก ๆ (ไม่ว่าจะเป็นผู้ให้หรือผู้รับ จะตั้งใจหรือไม่ก็ตาม)\n",
        "  - ฉะนั้น\n",
        "    - ให้แน่ใจว่า ไม่ดูโปรแกรมของคนอื่น\n",
        "    - ให้แน่ใจว่า ไม่ได้ให้คนอื่นดูโปรแกรมของตัวเอง\n",
        "\n",
        "- หากพบว่างานที่ส่งส่อทุจริต นิสิตจะได้ $0$ ใน<font color=red>การบ้านครั้งนี้ และการบ้านครั้งก่อนหน้าทั้งหมด</font>\n",
        "- จะตรวจให้คะแนน เมื่อ\n",
        " - แฟ้มที่ส่งครั้งหลังสุดภายในกำหนดส่งใน MyCourseVille เป็นแฟ้มที่ตั้งชื่อตามที่กำหนด และเป็นแฟ้มที่ได้จากการ File->Download->Download .ipynb เท่านั้น (ไม่ใช่แฟ้ม .py แล้วมา rename เป็น .ipynb)\n",
        " - ฟังก์ชันที่เขียนส่งต้องอยู่ใน code cell ที่ให้มาเท่านั้น\n",
        " - ไม่ทำในสิ่งที่ห้ามทำในข้อห้าม\n",
        " - code cell ที่ให้เขียนสามารถทำงานได้ (ไม่มี error)\n",
        " - ให้แน่ใจว่า code ที่ส่งมา ไม่มี syntax error, indentation error หรืออื่น ๆ ที่ทำให้สั่งทำงาน code ไม่ได้ (ถ้ามี ก็ตรวจไม่ได้ ได้ 0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DLvxhuA0NIzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**การส่งงาน**\n"
      ],
      "metadata": {
        "id": "lgsq1C6oOkg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ส่งไฟล์ในรูปแบบ .ipynb ที่ได้จากการเลือกเมนู File->Download->Download .ipynb ของ Colab และส่งแฟ้ม ipynb นี้ใน MyCourseVille\n",
        "2. ตั้งชื่อไฟล์เป็น HW4_XXXXXXXXX.ipynb เมื่อ XXXXXXXXXX คือ เลขประจำตัวนิสิต\n",
        "3. ส่งแฟ้ม .ipynb กี่ครั้งก็ได้ แต่จะตรวจแฟ้ม .ipynb **แฟ้มล่าสุด ที่ส่งภายในกำหนด** เท่านั้น\n",
        "4. <font color=\"red\">**กำหนดส่ง คือ ก่อน 23:59 น. ของพุธที่ 27 มีนาคม 2567**</font>"
      ],
      "metadata": {
        "id": "v7-Ib0uROq3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV File\n",
        "Comma-separated values หรือ CSV เป็นรูปแบบไฟล์ข้อความที่คั่นด้วยเครื่องหมาย comma (,) ซึ่งสามารถใช้แสดงข้อมูลในรูปแบบตารางได้\n",
        "\n",
        "* ตัวอย่างข้อมูลด้านล่างเป็นข้อมูลการโพสต์ข้อความทางโซเชียลมีเดียหลากหลายช่องทาง ในช่วงเวลาต่าง ๆ จากหลากหลายประเทศ\n",
        "* ทดลองรันเซลล์ด้านล่าง จะดาวน์โหลดไฟล์มาไว้ใน colab ของเรา\n",
        "* หรือจะดาวน์โหลดจากลิ้งค์นี้ก็ได้ https://drive.google.com/file/d/1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp/view?usp=share_link\n",
        "* อ้างอิงข้อมูลจาก Social Media Sentiments Analysis Dataset บน Kaggle โดยมีการปรับข้อมูลให้เหมาะกับโจทย์ในการบ้านนี้"
      ],
      "metadata": {
        "id": "aHC4vTRPWZtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp -O social_media_data.csv"
      ],
      "metadata": {
        "id": "ibgkZGdnY4SH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5da6400-009d-4739-962b-e91195833a18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 06:39:57--  https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp [following]\n",
            "--2024-03-24 06:39:57--  https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162403 (159K) [application/octet-stream]\n",
            "Saving to: ‘social_media_data.csv’\n",
            "\n",
            "social_media_data.c 100%[===================>] 158.60K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-03-24 06:39:58 (52.4 MB/s) - ‘social_media_data.csv’ saved [162403/162403]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* หากเปิดไฟล์ในโปรแกรม notepad หรือ edittext จะเห็นข้อมูลคั่นด้วย comma (,) ดังแสดงด้านล่าง\n",
        "\n",
        "![CSV file](https://mycourseville-default.s3.ap-southeast-1.amazonaws.com/useruploaded_course_files/2023_2/46110/materials/Screenshot_2567_03_09_at_14.00.36-465640-17099677538211.png)"
      ],
      "metadata": {
        "id": "C37ZR_29ceFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* แต่หากลองเปิดไฟล์ใน Colab (โดยการดับเบิ้ลคลิกที่ไฟล์ **`social_medial_data.csv`** ทางด้านซ้าย) หรือนำไปเปิดใน excel จะเห็นเป็นตารางด้านล่าง\n",
        "\n",
        "![CSV file](https://mycourseville-default.s3.ap-southeast-1.amazonaws.com/useruploaded_course_files/2023_2/46110/materials/Screenshot_2567_03_09_at_13.59.46-465640-17099676757348.png)"
      ],
      "metadata": {
        "id": "hd5H3HcTcnhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## งานของคุณ\n",
        "\n",
        "ให้เขียนฟังก์ชัน 6 ฟังก์ชันดังต่อไปนี้ เพื่ออ่านข้อความที่มีการโพสต์บนโซเชียลมีเดียที่รวบรวมอยู่ในชุดข้อมูลที่เตรียมไว้ให้ (social_media_data.csv) และหาคำที่พบบ่อยที่สุด เพื่อใช้ระบุเทรนในช่วงเวลานั้น ๆ\n",
        "\n",
        "* คะแนนฟังก์ชัน 1-5 ฟังก์ชันละ 4 คะแนน ฟังก์ชัน 6 จะเป็น 5 คะแนน\n",
        "\n"
      ],
      "metadata": {
        "id": "IIjf4uXwvJ7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **`social_media_data(file_in)`**\n",
        "\n",
        "รับค่า\n",
        "* **`file_in`**   คือชื่อไฟล์ของข้อมูลโซเชียลมีเดีย เช่น **`social_media_data.csv`**  แต่ไฟล์อาจจะเป็นชื่ออื่นก็ได้ แต่หัวตารางจะเป็นรูปแบบเดียวกันเสมอ ดูตัวอย่างตารางด้านบน\n",
        "\n",
        "คืนค่าเป็น **`Dict`** ของข้อมูลทั้งหมดที่อ่านจากไฟล์ที่มี\n",
        "* ค่า **`key`** เป็นค่าในคอลัมน์ Post_ID ในไฟล์ social_media_data.csv\n",
        "* **`value`** จะเป็นลิสต์ของข้อความ (คอลัมน์ Text), Platform, จำนวน retweet, จำนวน likes, ประเทศ, และปีที่โพสต์ หรือ\n",
        " **`[Text, Platform, Retweets, Likes, Country, Year]`**\n",
        "โดยจำนวน retweet และ likes จะต้องถูกเก็บในรูปแบบจำนวนเต็ม\n",
        "\n",
        "* อย่าลืมที่จะ **`strip()`** แต่ละข้อความที่อ่านมาด้วย\n",
        "* ในกรณีที่เกิด UnicodeDecodeError นิสิตอาจเปิดอ่านไฟล์โดยใช้ Unicode (UTF-8) encoding ดังตัวอย่าง\n",
        "```\n",
        "f = open(file_in, encoding=\"utf-8\")\n",
        "```\n",
        "\n",
        "ตัวอย่างเช่น\n",
        "* **`social_media_data(\"social_media_data.csv\")`**\n",
        "จะคืนค่าเป็น **`Dict`** ที่มีข้อมูลดังนี้\n",
        "```\n",
        "{'0000': ['Enjoying a beautiful day at the park!               #Nature #Park', 'Twitter', 19, 34, 'USA', '2023'],\n",
        "'0001': ['Traffic was terrible this morning.                  #Traffic #Morning', 'Twitter', 6, 18, 'Canada', '2023'],\n",
        "'0002': ['Just finished an amazing workout! 💪                #Fitness #Workout', 'Instagram', 21, 47, 'USA', '2023'],\n",
        "'0003': ['Excited about the upcoming weekend getaway!         #Travel #Adventure', 'Facebook', 9, 17, 'UK', '2023'],\n",
        "...,\n",
        "'0731':['Organizing a virtual talent show during challenging times bringing smiles to classmates' faces!  #VirtualEntertainment #HighSchoolPositivity\", 'Instagram', 24, 51, 'USA', '2020']}\n",
        "```\n",
        "\n",
        "* กรณีไม่มีข้อมูลจะในไฟล์ CSV (มีแต่หัวตาราง) จะคืนค่าเป็น **`Dict`** ว่าง\n",
        "```\n",
        "{}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1zBDQiMDIskp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **`is_stopword(word)`**\n",
        "\n",
        "* ฟังก์ชันตรวจสอบคำหรือ **`word`**  ที่รับเข้ามาว่าเป็น stopword หรือไม่ โดยเทียบกับคำที่เป็น stopword ที่ถูกเก็บไว้ในไฟล์ชื่อ stopwords.txt\n",
        "* Stop words คือคำทั่ว ๆ ไป ที่เราพบบ่อย ๆ ในประโยค หรือ เอกสาร แต่ไม่ค่อยช่วยในการสื่อความหมายนัก เช่น a, an, the, also, just, quite, unless, etc.\n",
        "\n",
        "* คืนค่าบูลลีน ซึ่งจะเป็นจริง เมื่อ **`word`** ที่รับเข้ามาเป็น stopword แต่หากไม่ใช่ จะคืนค่าเป็นเท็จ\n",
        "\n",
        "* หมายเหตุ กรณีคำ stopword ที่มีเครื่องหมายวรรคตอนระหว่างตัวอักษร เช่น can't, isn't, they'll เพื่อให้ง่าย ไม่ได้นำคำเหล่านี้มาคิดเป็น stopword แต่ให้เอาเครื่องหมายวรรคตอนออก และมองเป็นคำเดียว เช่น can't จะเป็นคำว่า cant, isn't เป็นคำว่า isnt, they'll จะเป็นคำกว่า theyll\n",
        "\n",
        "* ตัวอย่างเช่น\n",
        "<table>\n",
        "<tr>\n",
        "  <td>word</td><td>is_stopword(word)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"Happy\"</td><td>False</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"birthDay\"</td><td>False</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"on\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"whiCH\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"Into\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"After\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"AFTER\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"afTEr\"</td><td>True</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"can't\"</td><td>False</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\"cant\"</td><td>False</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "* ดาวน์โหลด stopwords.txt file ได้จากการรันเซลล์ด้านล่าง หรือจะดาวน์โหลดที่ลิ้งค์นี้ https://drive.google.com/file/d/1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh/view?usp=share_link"
      ],
      "metadata": {
        "id": "gP1GVJPpIi-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords.txt\n",
        "!wget https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh -O stopwords.txt"
      ],
      "metadata": {
        "id": "ozfbpG0MR5LU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555b8647-86a4-4573-99b0-8cb600d8a67c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 06:39:58--  https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh [following]\n",
            "--2024-03-24 06:39:58--  https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 761 [application/octet-stream]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "stopwords.txt       100%[===================>]     761  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:39:59 (66.2 MB/s) - ‘stopwords.txt’ saved [761/761]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **`count_words_from_text(word_count_dict, text)`**\n",
        "\n",
        "ฟังก์ชันนับคำจากข้อความหรือ **`text`** และอัพเดทคำที่นับได้ไปใน  **`Dict`** ที่รับเข้ามา **`word_count_dict`** โดยคำที่นับจะต้องไม่ใช่ Stop words\n",
        "\n",
        "รับค่า\n",
        "* **`word_count_dict`** ซึ่งเป็น **`Dict`** เก็บ key ที่เป็นคำศัพท์ตัวอักษรเล็กทั้งหมด และ value ที่เป็นจำนวนที่นับได้ก่อนหน้า\n",
        "* **`text`** คือข้อความที่จะนำมานับคำและอัพเดทเข้าไปใน **`word_count_dict`**\n",
        "\n",
        "คืนค่า\n",
        "* **`word_count_dict`** การนับคำก่อนหน้าและเพิ่มการนับคำใน **`text`**  เข้าไปด้วย\n",
        "\n",
        "แนะนำเพิ่มเติม\n",
        "* คำที่นับตัวเล็กตัวใหญ่ถือว่าเป็นคำเดียวกัน เช่น Enjoy และ enjoy ถือเป็นคำเดียวกัน\n",
        "* คำที่เป็นพหูพจน์จะถือว่าเป็นคนละคำ เช่น pig และ pigs จะมองเป็นคนละคำ\n",
        "* แนะนำให้เรียกใช้ **`is_stopword(word)`** เพื่อตรวจสอบคำว่าเป็น Stop words หรือไม่ เช่น a เป็น stop word <mark>ไม่</mark>ควรนำมานับด้วย\n",
        "* หากมีการตัดเป็นคำ แนะนำให้ระวังเครื่องหมายวรรคตอนด้วย ได้แก่ .,:;?()[]\"'{}-/\\|_!\n",
        "* คำที่ติด Hashtag (#) คือเป็นคนละคำกับคำที่ไม่ติด # เช่น #Enjoy คือคนละคำกับ Enjoy (จะไม่มี # อยู่เดี่ยว ๆ)\n",
        "* หากเจอคำที่มีเครื่องหมายวรรคตอนคั่นกลาง เพื่อให้ง่ายให้ตัดเครื่องหมายวรรคตอนออก เช่น soul-stirring ให้ใช้เป็นคำศัพท์ว่า soulstirring, Jay-Z ใช้เป็นคำว่า jayz, Can't ใช้เป็นคำว่า cant\n",
        "* ลำดับ key ที่แสดงผลจาก **`word_count_dict`**  ไม่จำเป็นต้องเรียงค่า\n",
        "\n",
        "\n",
        "ตัวอย่างเช่น\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>word_count_dict</td>\n",
        "  <td>text</td>\n",
        "  <td>count_words_from_text(word_count_dict, text) -> ลำดับสลับได้</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 1, 'market': 1}</td>\n",
        "  <td>\"Hello today! #Happy\"</td>\n",
        "  <td>{'aroma': 1, 'market': 1, 'hello': 1, 'today': 1, '#happy': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 1, 'market': 1}</td>\n",
        "  <td>\"Exploring the local market today, can't wait to share with everyone!\"</td>\n",
        "  <td>{'aroma': 1, 'market': 2, 'exploring': 1, 'local': 1, 'today': 1, 'cant': 1, 'wait': 1, 'share': 1, 'everyone': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 1, 'market': 1}</td>\n",
        "  <td>\"LoVe Aroma THeRapy!! LoVed it LoVe it #AROMA.\"</td>\n",
        "  <td>{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{}</td>\n",
        "  <td>\"Mary has \"A little Lamb\", LiTTLE Lamb, MARY has a Little cat!! #lamb #cat\"</td>\n",
        "  <td>{'mary': 2, 'little': 3, 'lamb': 2, 'cat': 1, '#lamb': 1, '#cat': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{}</td>\n",
        "  <td>\"Mary has A lot of Lambs, a little lamb.\"</td>\n",
        "  <td>{'mary': 1, 'lot': 1, 'lambs': 1, 'little': 1, 'lamb': 1}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'mary': 2, 'little': 3}</td>\n",
        "  <td> \"Twinkle twinkle 'little' star..... How I wonder what you are.\"</td>\n",
        "  <td>{'mary': 2, 'little': 4, 'twinkle': 2, 'star': 1, 'wonder': 1}</td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "AnGiqJ9iI1Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  **`count_words_from_data_dict(data_dict, year, country, platform)`**\n",
        "\n",
        "ฟังก์ชันที่นับจำนวนคำจาก\n",
        "* **`data_dict`** ที่เก็บข้อมูลในรูปแบบ **`Dict`** ที่มี **`key`**  เป็น **`Post_ID`** และ **`value`** จะเป็นลิสต์ของข้อความ (คอลัมน์ Text), Platform, จำนวน retweet, จำนวน likes, ประเทศ, และปีที่โพสต์ หรือ\n",
        " **`[Text, Platform, Retweets, Likes, country, Year]`**\n",
        "โดยจำนวน retweet และ likes จะต้องถูกเก็บในรูปแบบจำนวนเต็ม\n",
        "* **`year`** ใส่เป็นข้อความระบุปีใดปีหนึ่ง เช่น 2023 แต่อาจจะใส่ค่า 'all' ได้ด้วยหมายถึงพิจารณาข้อมูลจากทุก ๆ ปี\n",
        "* **`country`** ใส่เป็นชื่อประเทศใดประเทศหนึ่ง เช่น USA แต่อาจจะใส่ค่า 'all' ได้ด้วยหมายถึงพิจารณาข้อมูลจากทุก ๆ ประเทศ\n",
        "* **`platform`** ใส่เป็นชื่อแพลตฟอร์มใดประเทศหนึ่ง เช่น Facebook แต่อาจจะใส่ค่า 'all' ได้ด้วยหมายถึงพิจารณาข้อมูลจากทุก ๆ แพลตฟอร์ม\n",
        "\n",
        "คืนค่าเป็น **`Dict`** ที่นับคำทุกคำในข้อมูลตามปี ประเทศ และแพลตฟอร์มที่กำหนด\n",
        "\n",
        "เพิ่มเติม\n",
        "* สามารถเรียกใช้ **`count_words_from_text(word_count_dict, text)`**\n",
        "* ต้องพิมพ์ตัวอักษรตัวเล็กตัวใหญ่ให้ถูกต้องสำหรับ year, country, และ platform เช่น ถ้าพิมพ์ <mark>J</mark>apan เป็น <mark>j</mark>apan จะไม่มีข้อมูลแสดงออกมา (เพราะข้อมูลมีแต่ <mark>J</mark>apan)\n",
        "\n",
        "ตัวอย่างเช่น\n",
        "* **`data_dict`**  จากฟังก์ชัน\n",
        "  * **`data_dict = social_media_data(\"social_media_data.csv\")`** จะได้ค่าเป็น\n",
        "  ```\n",
        "{'0000': ['enjoying a beautiful day at the park!               #nature #park', 'Twitter', 19, 34, 'USA', '2023'],\n",
        "'0001': ['traffic was terrible this morning.                  #traffic #morning', 'Twitter', 6, 18, 'Canada', '2023'],\n",
        "'0002': ['just finished an amazing workout! 💪                #fitness #workout', 'Instagram', 21, 47, 'USA', '2023'],\n",
        "'0003': ['excited about the upcoming weekend getaway!         #travel #adventure', 'Facebook', 9, 17, 'UK', '2023'],\n",
        "...,\n",
        "'0731':['\"organizing a virtual talent show during challenging times bringing smiles to classmates' faces!  #virtualentertainment #highschoolpositivity\", 'Instagram', 24, 51, 'USA', '2020']}\n",
        "```\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>year</td><td>country</td><td>platform</td><td>count_words_from_data_dict(data_dict, year, country, platform)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'all'</td><td>'all'</td><td>'all'</td><td>{'enjoying': 5, 'beautiful': 4, 'day': 26, 'park': 3, ... } (จำนวน 3446 คำ) </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2023'</td><td>'USA'</td><td>'Facebook'</td><td>{'reflecting': 1, 'past': 1, 'looking': 1, 'ahead': 1, '#reflection': 1, ... } (จำนวน 313 คำ) </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'all'</td><td>'Thailand'</td><td>'all'</td><td>{'heart': 1, 'bustling': 1, 'market': 1, 'street': 1, 'food': 1, 'connoisseur': 1, 'indulges': 1, 'culinary': 1, 'adventure': 1, 'savoring': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2023'</td><td>'Thailand'</td><td>'all'</td><td>{}\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2020'</td><td>'Japan'</td><td>'Twitter'</td><td>{'avoiding': 1, 'shards': 1, 'shattered': 1, 'dreams': 1, 'walking': 1, 'tightrope': 1, 'resilience': 1, '#resilience': 1, '#tightropewalk': 1}\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>'2020'</td><td>'<mark>j</mark>apan'</td><td>'<mark>t</mark>witter'</td><td>{}\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "GSNkn5JMJEhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **`top_k_words(word_count_dict, k)`**\n",
        "คืนค่าลิสต์ของคำที่มีจำนวนมากที่สุด k จำนวน <mark>เรียงจากจำนวนที่ปรากฏมากที่สุดไปน้อยที่สุด\n",
        "หากมีจำนวนเท่ากันจะเรียงลำดับตามตัวอักษร </mark>\n",
        "\n",
        "รับค่า\n",
        "* **`word_count_dict`** เป็น **`Dict`** ที่เก็บคำและจำนวนการปรากฏของแต่ละคำ\n",
        "* **`k`** จะเป็นอันดับสูงสุด k อันดับ\n",
        "\n",
        "การคืนค่า\n",
        "* คืนลิสต์ที่เรียงลำดับจากคำที่มีจำนวนมากสุดไปน้อยสุด แต่หากมีจำนวนเท่ากันให้เรียงคำตามลำดับตัวอักษรตามพจนานุกรม\n",
        "  * แต่หากอันดับสูงสุดมีมากกว่า k ค่า เช่น ต้องการ 3 คำลำดับสูงสุด ใน **`Dict`**  ตัวอย่างด้านล่าง มีอันดับ 2 ถึง 4 คำ ก็จะคืนลิสต์ของคำที่ซ้ำอันดับที่ 2 ทั้งหมด\n",
        "```\n",
        " {'dreams':15,'like':10,'feeling':10,'journey':10,'sky':10,'challenges':9,'day':9,'new':9,'world':9}\n",
        " ```\n",
        "    * จะได้ผลลัพธ์เป็น **`['dreams', 'feeling', 'journey', 'like', 'sky']`** ลำดับที่ 2 ซ้ำ 4 คำ จึงแสดงผลทั้งหมดเลย\n",
        "  * หากอันดับสูงสุดที่ต้องการมีมากกว่าคำที่มี ใหคืนลิสต์คำเท่าที่มี เรียงลำดับตามพจนานุกรม\n",
        "  \n",
        "ตัวอย่างเช่น\n",
        "<table>\n",
        "<tr>\n",
        "  <td>word_count_dict</td>\n",
        "  <td>k</td>\n",
        "  <td>top_k_words(word_count_dict, k)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22}</td><td>5</td>\n",
        "  <td>['new', 'like', 'day', 'feeling', 'dreams']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22}</td><td>3</td>\n",
        "  <td>['new', 'like', 'day', 'feeling']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}<td>5</td>\n",
        "  <td>['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}<td>3</td>\n",
        "  <td>['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'market': 1, 'aroma': 1}<td>5</td>\n",
        "  <td>['aroma', 'market']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'market': 1, 'aroma': 1}<td>3</td>\n",
        "  <td>['aroma', 'market']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}<td>5</td>\n",
        "  <td>['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}<td>3</td>\n",
        "  <td>['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']</td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "4xZOmztQI75Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  **count_word_summary(file_in, file_out, k, year, country, platform)**\n",
        "\n",
        "ฟังก์ชันที่อ่านข้อมูลในไฟล์ นับจำนวนคำและเขียนสรุปคำที่ปรากฏบ่อยที่สุด k จำนวน ใน ปี ประเทศ และแพลตฟอร์มที่กำหนด\n",
        "\n",
        "รับค่า\n",
        "* ชื่อไฟล์ **`file_in`** ที่เป็นไฟล์ CSV ที่มีหัวตารางเหมือนไฟล์ **`social_medial_data.csv`**\n",
        "* **`file_out`** ชื่อไฟล์ output (เป็น .txt) ที่จะสรุปคำที่ปรากฏบ่อยสุด จะเป็นอันดับสูงสุด **`k`** คำ ดูรูปแบบการเขียนในตัวอย่าง (ดุเพิ่มเติมเกี่ยวกับ **`k`**ในฟังก์ชัน 5.)\n",
        "* **`year`** ใส่เป็นข้อความระบุปีใดปีหนึ่ง เช่น 2023 แต่อาจจะใส่ค่า 'all' ได้ด้วยหมายถึงพิจารณาข้อมูลจากทุก ๆ ปี\n",
        "* **`country`** ใส่เป็นชื่อประเทศใดประเทศหนึ่ง เช่น USA แต่อาจจะใส่ค่า 'all' ได้ด้วยหมายถึงพิจารณาข้อมูลจากทุก ๆ ประเทศ\n",
        "* **`platform`** ใส่เป็นชื่อแพลตฟอร์มใดประเทศหนึ่ง เช่น Facebook แต่อาจจะใส่ค่า 'all' ได้ด้วยหมายถึงพิจารณาข้อมูลจากทุก ๆ แพลตฟอร์ม\n",
        "\n",
        "<mark>ไม่มีการคืนค่า</mark>\n",
        "\n",
        "แนะนำเพิ่มเติม\n",
        "* แนะนำให้ใช้ฟังก์ชัน 1-5 ก่อนจะนำมาสร้างไฟล์สรุป\n",
        "* ต้องพิมพ์ตัวอักษรตัวเล็กตัวใหญ่ให้ถูกต้องสำหรับ year, country, และ platform เช่น ถ้าพิมพ์ <mark>J</mark>apan เป็น <mark>j</mark>apan จะแสดงผลในไฟล์ **`file_out`** ว่า No data(เพราะข้อมูลมีแต่ <mark>J</mark>apan)\n",
        "\n",
        "ตัวอย่างเช่น\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>file_in</td><td>file_out</td><td>k</td><td>year</td><td>country</td><td>platform</td><td>ข้อมูลใน file_out</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'all'</td><td>'all'</td><td>'all'</td>\n",
        "  <td>new:43</br>like:27</br>day:26</br>feeling:26</br>dreams:25</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'all'</td><td>'all'</td><td>'all'</td>\n",
        "  <td>new:43</br>like:27</br>day:26</br>feeling:26</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'2023'</td><td>'USA'</td><td>'Facebook'</td>\n",
        "  <td>art:4</br>every:3</br>experience:3</br>exploring:3</br>local:3</br>love:3</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'2023'</td><td>'USA'</td><td>'Facebook'</td>\n",
        "  <td>art:4</br>every:3</br>experience:3</br>exploring:3</br>local:3</br>love:3</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'all'</td><td>'Thailand'</td><td>'all'</td>\n",
        "  <td>#culinaryadventure:1</br>#streetfooddelights:1</br>adventure:1</br>aromas:1</br>bustling:1</br>connoisseur:1</br>culinary:1</br>diverse:1\n",
        "  </br>flavors:1\n",
        "  </br>food:1\n",
        "  </br>heart:1\n",
        "  </br>indulges:1\n",
        "  </br>market:1\n",
        "  </br>savoring:1\n",
        "  </br>street:1\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'2023'</td><td>'Thailand'</td><td>'all'</td>\n",
        "  <td>No data\n",
        "  </td>\n",
        "  </tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'2023'</td><td>'UK'</td><td>'Instagram'</td>\n",
        "  <td>new:7</br>adventure:4</br>friends:4</br>art:3</br>day:3</br>magic:3</br>weekend:3</br>wine:3\n",
        "  </br>years:3\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>3</td><td>'all'</td><td>'India'</td><td>'all'</td>\n",
        "  <td>day:5</br>new:5</br>painting:5</br>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>social_medial_data.csv</td><td>summary.txt</td><td>5</td><td>'all'</td><td>'India'</td><td>'all'</td>\n",
        "  <td>day:5</br>new:5</br>painting:5\n",
        "  </br>#gratitude:4\n",
        "  </br>#hopeful:4\n",
        "  </br>dreams:4\n",
        "  </br>hopeful:4\n",
        "  </br>life:4\n",
        "  </br>optimism:4\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "393bjIKfJI79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HW4_Social_Media_Data\n",
        "\n",
        "# Input CSV file and return all data in CSV file as a dict\n",
        "# Key: Post_ID\n",
        "# Value: [Text, Platform, Retweets, Likes, Country, Year]\n",
        "# From social_media_data.csv header, we have\n",
        "# Post_ID,Text,Sentiment,Timestamp,User,Platform,Retweets,Likes,Country,Year,Month,Day,Hour\n",
        "# Since the first line of the CSV file is the header, we need to skip it.\n",
        "# After skip it, we need to split data with comma (,) and input an a data in index 1,5,6,7,8,9\n",
        "# Don't forget to convert Retweets and Likes to integer.\n",
        "def social_media_data(file_in):\n",
        "    f = open(file_in, encoding=\"utf-8\")\n",
        "    social_dict = {}\n",
        "    IsFirstLine = True\n",
        "    for line in f:\n",
        "        # Skip first line (Header)\n",
        "        if(not IsFirstLine):\n",
        "            data = line.strip().split(',')\n",
        "            social_dict[data[0]] = [data[1].strip(), data[5].strip(),\n",
        "                                    int(data[6]), int(data[7]),\n",
        "                                    data[8].strip(), data[9].strip()]\n",
        "        else:\n",
        "            IsFirstLine = False\n",
        "    return social_dict\n",
        "\n",
        "# Check if a parameter 'word' is in the file stopwords.txt\n",
        "# Consider lowercase and uppercase as a same character\n",
        "def is_stopword(word):\n",
        "    f = open(\"stopwords.txt\", \"r\")\n",
        "    # Get a list of stopwords from stopwords.txt\n",
        "    stopwords = f.readline().strip().split(\", \")\n",
        "    for i in range(len(stopwords)):\n",
        "        stopwords[i] = stopwords[i].strip()\n",
        "    # Check if the word is stopword\n",
        "    if(word.lower() in stopwords):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Count a word (except stopwords) in text and update them in word_count_dict\n",
        "# - Consider lowercase and uppercase as a same character\n",
        "# - Consider singular and plural word as different word\n",
        "# - Consider hashtag word as different word (Example: #Enjoy and enjoy is not the same)\n",
        "#   and guaruntee that there are no single '#' in a text\n",
        "# - Consider a word like \"can't\" --> cant, \"soul-stirring\" --> \"soulstirring\" and \"Jay-Z\" --> \"jayz\"\n",
        "# - Beware of \".,:;?()[]\"'{}-/|_!\"\n",
        "# - No need to order keys in word_count_dict\n",
        "def count_words_from_text(word_count_dict, text):\n",
        "    # Split a string into a list of words\n",
        "    words = text.split()\n",
        "    # Convert all words in a list into lowercase and remove all symbols\n",
        "    for i in range(len(words)):\n",
        "        words[i] = words[i].lower()\n",
        "        temp = \"\"\n",
        "        for char in words[i]:\n",
        "            if(char not in \".,:;?()[]\\\"\\'{}-/|_!\"):\n",
        "                temp += char\n",
        "        words[i] = temp\n",
        "    # Count words\n",
        "    for word in words:\n",
        "        if(not is_stopword(word)):\n",
        "            if(word not in word_count_dict):\n",
        "                word_count_dict[word] = 1\n",
        "            else:\n",
        "                word_count_dict[word] += 1\n",
        "    return word_count_dict\n",
        "\n",
        "# This function filter a data by year, country and platform and returns as a word count dict\n",
        "# Parameter 'data_dict' keys is 'Post_ID' and value is [Text, Platform, Retweets, Likes, Country, Year]\n",
        "# Count words in 'Text' in each value in data_dict\n",
        "# If year = 'all', output data in all year from a data_dict\n",
        "# If country = 'all', output data in all country from data_dict\n",
        "# If platform = 'all', output data in all platform from\n",
        "# Uppercase and lowercase are not the same\n",
        "def count_words_from_data_dict(data_dict, year, country, platform):\n",
        "    words_count = {}\n",
        "    for key in data_dict:\n",
        "        # All year, country and platform\n",
        "        if(year == 'all' and country == 'all' and platform == 'all'):\n",
        "            words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All year and country. Filter only platform\n",
        "        elif(year == 'all' and country == 'all'):\n",
        "            if(data_dict[key][1] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All year and platform. Filter only country\n",
        "        elif(year == 'all' and platform == 'all'):\n",
        "            if(data_dict[key][4] == country):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All country and platform. Filter only year\n",
        "        elif(country == 'all' and platform == 'all'):\n",
        "            if(data_dict[key][5] == year):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All year. Filter country and platform\n",
        "        elif(year == 'all'):\n",
        "            if(data_dict[key][4] == country and data_dict[key][5] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All country. Filter year and platform\n",
        "        elif(country == 'all'):\n",
        "            if(data_dict[key][5] == year and data_dict[key][1] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # All platform. Filter year and country\n",
        "        elif(platform == 'all'):\n",
        "            if(data_dict[key][5] == year and data_dict[key][4] == country):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "        # Filter year, country and platform\n",
        "        else:\n",
        "            if(data_dict[key][5] == year and data_dict[key][4] == country and data_dict[key][1] == platform):\n",
        "                words_count = count_words_from_text(words_count,data_dict[key][0])\n",
        "    return words_count\n",
        "\n",
        "# This function returns top 'k' highest word count. Returns as a list with words\n",
        "# If the words have the same count, then sort them in alphabetical order\n",
        "# In case of k is greater than len(unique_count), returns all words but in order\n",
        "# In case of length of list 'words' is already greater than k, break the loop\n",
        "def top_k_words(word_count_dict, k):\n",
        "    # Find unique_count from word_count_dict\n",
        "    # Example: word_count_dict = {'dreams':15,,'day':9,'like':10,'feeling':10,'challenges':9}\n",
        "    # unique_count = [15,10,9]\n",
        "    unique_count = []\n",
        "    for key in word_count_dict:\n",
        "        if(word_count_dict[key] not in unique_count):\n",
        "            unique_count.append(word_count_dict[key])\n",
        "    unique_count.sort(reverse = True)\n",
        "    # Find and return a list of top 'k' highest word count\n",
        "    words = []\n",
        "    # Use min(k, len(unique_count) in case of k is greater than len(unique_count)\n",
        "    for i in range(min(k, len(unique_count))):\n",
        "        temp = []\n",
        "        if(len(words) < k):\n",
        "            for key in word_count_dict:\n",
        "                if(word_count_dict[key] == unique_count[i]):\n",
        "                    temp.append(key)\n",
        "            temp.sort()\n",
        "            for item in temp:\n",
        "                words.append(item)\n",
        "        # Break the loop in case of length of list 'words' is already greater than k\n",
        "        else:\n",
        "            break\n",
        "    return words\n",
        "\n",
        "# Use all previous function to solve this\n",
        "# - file_in = social_media_data.csv\n",
        "# - file_out = top 'k' word count (.txt)\n",
        "# - year = if 'all', then consider all year\n",
        "# - country = if 'all', then consider all country\n",
        "# - platform = if 'all', then consider all platform\n",
        "# - This function does not return anything but edit/create file in parameter 'file_out'\n",
        "# - If there is no data, then edit text in 'file_out' as 'No data'\n",
        "def count_word_summary(file_in, file_out, k, year, country, platform):\n",
        "    social_dict = social_media_data(file_in)\n",
        "    count_words = count_words_from_data_dict(social_dict, year, country, platform)\n",
        "    top_k = top_k_words(count_words, k)\n",
        "    f = open(file_out, 'w')\n",
        "    text = \"\"\n",
        "    for word in top_k:\n",
        "        text += word + \":\" + str(count_words[word]) + \"\\n\"\n",
        "    if(text == \"\"):\n",
        "        f.write(\"No data\")\n",
        "    else:\n",
        "        f.write(text)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "32TQlQREdOVp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **กรณีทดสอบ**\n",
        "\n",
        "1. กรณีทดสอบไหนที่ run แล้วผ่าน จะเห็นข้อความ `... ok`\n",
        "\n",
        "เช่น\n",
        "```\n",
        "test_1_social_media_data (__main__.TestHW4) ... ok\n",
        "test_2_is_stopword (__main__.TestHW4) ... ok\n",
        "test_3_count_words_from_text (__main__.TestHW4) ... ok\n",
        "test_4_count_words_from_data_dict (__main__.TestHW4) ... ok\n",
        "test_5_top_k_words (__main__.TestHW4) ... ok\n",
        "test_6_count_word_summary (__main__.TestHW4) ... ok\n",
        "```\n",
        "หมายถึงทั้ง 6 ฟังก์ชันทำงานได้ถูกต้อง\n",
        "\n",
        "\n",
        "2. กรณีทดสอบฟังก์ชันใดที่รันแล้ว เห็นข้อความ `... skipped` แสดงว่ายังไม่ได้เขียนฟังก์ชันนั้น เช่น\n",
        "\n",
        "```\n",
        "test_3_count_words_from_text (__main__.TestHW4) ... skipped '\"count_words_from_text()\" is not defined or not implemented'\n",
        "```\n",
        "\n",
        "แสดงว่ายังไม่ได้เขียนฟังก์ชันที่ 3 ฟังก์ชัน count_words_from_text()\n",
        "\n",
        "\n",
        "\n",
        "3. กรณีหากฟังก์ชั่นใด<mark>ได้ผลไม่ถูกต้อง</mark> run แล้วจะเจอข้อความดังตัวอย่างข้างล่างนี้\n",
        "เคสนั้น ๆ จะ<mark>ไม่มีคำว่า ok และ skipped</mark>\n",
        "```\n",
        "test_6_count_word_summary (__main__.TestHW4) ...\n",
        "```\n",
        "และมีรายละเอียด\n",
        "\n",
        "```\n",
        "======================================================================\n",
        "FAIL: test_6_count_word_summary (__main__.TestHW4) (i=1)\n",
        "----------------------------------------------------------------------\n",
        "Traceback (most recent call last):\n",
        "  File \"/hw4_testcases.py\", line 205, in test_6_count_word_summary\n",
        "    self.assertEqual(\"\".join(f.readlines()).strip(), test_case['expected'])\n",
        "AssertionError: 'new:43\\nlike:27\\nday:26\\nfeeling:26' != 'new:43\\nlike:27\\nday:26\\nfeeling:27'\n",
        "  new:43\n",
        "  like:27\n",
        "  day:26\n",
        "- feeling:26?          ^\n",
        "+ feeling:27?          ^\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "หมายถึง ใน `test_6_count_word_summary` .ให้ผลลัพธ์ไม่ตรงกับที่ควรจะเป็นในกรณีทดสอบย่อยลำดับที่ 2 (ดูตรง i=1 ลำดับแรกเริ่มที่ i = 0)\n"
      ],
      "metadata": {
        "id": "Ofeef8wy_qxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download files ที่ต้องใช้ในการทดสอบด้านล่างก่อน"
      ],
      "metadata": {
        "id": "4sFUo5g9AXhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp -O social_media_data.csv\n",
        "!wget https://drive.google.com/uc?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX -O empty_data.csv\n",
        "!wget https://drive.google.com/uc?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW -O 4_1.txt\n",
        "!wget https://drive.google.com/uc?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y -O 4_2.txt\n",
        "!wget https://drive.google.com/uc?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz -O 1_1.txt\n",
        "!wget https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh -O stopwords.txt"
      ],
      "metadata": {
        "id": "Sv1oFeHlAa2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef30a2e-e907-4e2c-ca0e-849865eb6612"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 06:39:59--  https://drive.google.com/uc?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp [following]\n",
            "--2024-03-24 06:39:59--  https://drive.usercontent.google.com/download?id=1N_6buJnQp6M-dQdaC1hVPPX5XDWizjNp\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162403 (159K) [application/octet-stream]\n",
            "Saving to: ‘social_media_data.csv’\n",
            "\n",
            "social_media_data.c 100%[===================>] 158.60K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-03-24 06:39:59 (111 MB/s) - ‘social_media_data.csv’ saved [162403/162403]\n",
            "\n",
            "--2024-03-24 06:39:59--  https://drive.google.com/uc?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX [following]\n",
            "--2024-03-24 06:39:59--  https://drive.usercontent.google.com/download?id=1sHkrgk-HL1EcO1ICXIEheTv7mENaoiSX\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89 [application/octet-stream]\n",
            "Saving to: ‘empty_data.csv’\n",
            "\n",
            "empty_data.csv      100%[===================>]      89  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:39:59 (2.06 MB/s) - ‘empty_data.csv’ saved [89/89]\n",
            "\n",
            "--2024-03-24 06:39:59--  https://drive.google.com/uc?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW [following]\n",
            "--2024-03-24 06:39:59--  https://drive.usercontent.google.com/download?id=1CqL0a52Tc1UmJyCtd3rKG6CllmEnCBwW\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62951 (61K) [application/octet-stream]\n",
            "Saving to: ‘4_1.txt’\n",
            "\n",
            "4_1.txt             100%[===================>]  61.48K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-03-24 06:40:00 (73.4 MB/s) - ‘4_1.txt’ saved [62951/62951]\n",
            "\n",
            "--2024-03-24 06:40:00--  https://drive.google.com/uc?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y [following]\n",
            "--2024-03-24 06:40:00--  https://drive.usercontent.google.com/download?id=1TDTtQF5FgzqYJs5HprnXRAJsXzqXMJ4y\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5497 (5.4K) [application/octet-stream]\n",
            "Saving to: ‘4_2.txt’\n",
            "\n",
            "4_2.txt             100%[===================>]   5.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:40:00 (71.1 MB/s) - ‘4_2.txt’ saved [5497/5497]\n",
            "\n",
            "--2024-03-24 06:40:01--  https://drive.google.com/uc?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz [following]\n",
            "--2024-03-24 06:40:01--  https://drive.usercontent.google.com/download?id=1xTJOuNmJclFd3P28hS__FkxNL02CQhfz\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 122118 (119K) [application/octet-stream]\n",
            "Saving to: ‘1_1.txt’\n",
            "\n",
            "1_1.txt             100%[===================>] 119.26K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-03-24 06:40:01 (64.9 MB/s) - ‘1_1.txt’ saved [122118/122118]\n",
            "\n",
            "--2024-03-24 06:40:01--  https://drive.google.com/uc?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.214.113, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.214.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh [following]\n",
            "--2024-03-24 06:40:01--  https://drive.usercontent.google.com/download?id=1iDOFnkVscGQppyoydDyxOCz8AUJd0oUh\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.148.132, 2607:f8b0:4001:c54::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.148.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 761 [application/octet-stream]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "stopwords.txt       100%[===================>]     761  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 06:40:02 (62.8 MB/s) - ‘stopwords.txt’ saved [761/761]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "def is_function_defined(func_name):\n",
        "    return func_name in globals()\n",
        "\n",
        "def skip_if_not_implemented(func_name):\n",
        "    def decorator(test_func):\n",
        "        if not is_function_defined(func_name):\n",
        "            return unittest.skip(\"\\\"\" + func_name + \"()\\\" is not defined or not implemented\")(test_func)\n",
        "        return test_func\n",
        "    return decorator\n",
        "\n",
        "\n",
        "class TestHW4(unittest.TestCase):\n",
        "\n",
        "    @skip_if_not_implemented('social_media_data')\n",
        "    def test_1_social_media_data(self):\n",
        "        test_case = {'args':'social_media_data.csv', 'expected':'1_1.txt'}\n",
        "        result = social_media_data(test_case['args'])\n",
        "        fsol = open(test_case['expected'],'r',encoding=\"utf-8\")\n",
        "        self.assertEqual(str(sorted(result.items())), \"\".join(fsol.readlines()))\n",
        "        fsol.close()\n",
        "\n",
        "        test_case = {'args':'empty_data.csv', 'expected': {}}\n",
        "        result = social_media_data(test_case['args'])\n",
        "        self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "\n",
        "    @skip_if_not_implemented('is_stopword')\n",
        "    def test_2_is_stopword(self):\n",
        "        test_cases = [\n",
        "            {'args':\"Happy\", 'expected':False},\n",
        "            {'args':\"birthDay\", 'expected':False},\n",
        "            {'args':\"on\", 'expected':True},\n",
        "            {'args':\"whiCH\", 'expected':True},\n",
        "            {'args':\"Into\", 'expected':True},\n",
        "            {'args':\"After\", 'expected':True},\n",
        "            {'args':\"AFTER\", 'expected':True},\n",
        "            {'args':\"afTEr\", 'expected':True},\n",
        "            {'args':\"can't\", 'expected':False},\n",
        "            {'args':\"cant\", 'expected':False}\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                result = is_stopword(test_case['args'])\n",
        "                self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('count_words_from_text')\n",
        "    def test_3_count_words_from_text(self):\n",
        "        test_cases = [\n",
        "            {'args':[{'aroma': 1, 'market': 1},\"Hello today! #Happy\"], 'expected':{'aroma': 1, 'market': 1, 'hello': 1, 'today': 1, '#happy': 1}},\n",
        "            {'args':[{'aroma': 1, 'market': 1},\"Exploring the local market today, can't wait to share with everyone!\"], 'expected':{'aroma': 1, 'market': 2, 'exploring': 1, 'local': 1, 'today': 1, 'cant': 1, 'wait': 1, 'share': 1, 'everyone': 1}},\n",
        "            {'args':[{'aroma': 1, 'market': 1},\"LoVe Aroma THeRapy!! LoVed it LoVe it #AROMA.\"], 'expected':{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1}},\n",
        "            {'args':[{},\"Mary has \\\"A little Lamb\\\", LiTTLE Lamb, MARY has a Little cat!! #lamb #cat\"], 'expected':{'mary': 2, 'little': 3, 'lamb': 2, 'cat': 1, '#lamb': 1, '#cat': 1}},\n",
        "            {'args':[{},\"Mary has A lot of Lambs, a little lamb.\"], 'expected':{'mary': 1, 'lot': 1, 'lambs': 1, 'little': 1, 'lamb': 1}},\n",
        "            {'args':[{'mary': 2, 'little': 3},\"Twinkle twinkle 'little' star..... How I wonder what you are.\"], 'expected':{'mary': 2, 'little': 4, 'twinkle': 2, 'star': 1, 'wonder': 1}},\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                d,t = test_case['args']\n",
        "                result = count_words_from_text(d,t)\n",
        "                self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('count_words_from_data_dict')\n",
        "    def test_4_count_words_from_data_dict(self):\n",
        "        test_cases = [\n",
        "            {'args':['all','all','all'], 'expected':'4_1.txt'},\n",
        "            {'args':['2023','USA','Facebook'], 'expected':'4_2.txt'},\n",
        "            {'args':['all','Thailand','all'], 'expected':{'heart': 1, 'bustling': 1, 'market': 1, 'street': 1, 'food': 1, 'connoisseur': 1, 'indulges': 1, 'culinary': 1, 'adventure': 1, 'savoring': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1}},\n",
        "            {'args':['2023','Thailand','all'], 'expected':{}},\n",
        "            {'args':['2020','Japan','Twitter'], 'expected':{'avoiding': 1, 'shards': 1, 'shattered': 1, 'dreams': 1, 'walking': 1, 'tightrope': 1, 'resilience': 1, '#resilience': 1, '#tightropewalk': 1}},\n",
        "            {'args':['2020','japan','twitter'], 'expected':{}},\n",
        "            ]\n",
        "        data_dict = social_media_data(\"social_media_data.csv\")\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                y,c,p = test_case['args']\n",
        "                result = count_words_from_data_dict(data_dict,y,c,p)\n",
        "                if i < 2:\n",
        "                    fsol = open(test_case['expected'],'r',encoding=\"utf-8\")\n",
        "                    self.assertEqual(str(sorted(result.items())), \"\".join(fsol.readlines()))\n",
        "                    fsol.close()\n",
        "                else:\n",
        "                    self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('top_k_words')\n",
        "    def test_5_top_k_words(self):\n",
        "        test_cases = [\n",
        "            {'args':[{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22},5], 'expected':['new', 'like', 'day', 'feeling', 'dreams']},\n",
        "            {'args':[{'new':43,'like':27,'day':26,'feeling':26,'dreams':25,'heart':24,'laughter':24,'joy':23,'night':23,'life':22},3], 'expected':['new', 'like', 'day', 'feeling']},\n",
        "            {'args':[{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1},5], 'expected':['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']},\n",
        "            {'args':[{'heart': 1, 'food': 1, 'diverse': 1, 'flavors': 1, 'aromas': 1, '#culinaryadventure': 1, '#streetfooddelights': 1},3], 'expected':['#culinaryadventure', '#streetfooddelights', 'aromas', 'diverse', 'flavors', 'food', 'heart']},\n",
        "            {'args':[{'market': 1, 'aroma': 1},5], 'expected':['aroma', 'market']},\n",
        "            {'args':[{'market': 1, 'aroma': 1},3], 'expected':['aroma', 'market']},\n",
        "            {'args':[{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1},5], 'expected':['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']},\n",
        "            {'args':[{'aroma': 2, 'market': 1, 'love': 2, 'therapy': 1, 'loved': 1, '#aroma': 1},3], 'expected':['aroma', 'love', '#aroma', 'loved', 'market', 'therapy']},\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                d,k = test_case['args']\n",
        "                result = top_k_words(d,k)\n",
        "                self.assertEqual(result, test_case['expected'])\n",
        "\n",
        "    @skip_if_not_implemented('count_word_summary')\n",
        "    def test_6_count_word_summary(self):\n",
        "        test_cases = [\n",
        "            {'args':['social_media_data.csv','summary.txt',5,'all','all','all'], 'expected':'new:43\\nlike:27\\nday:26\\nfeeling:26\\ndreams:25'},\n",
        "            {'args':['social_media_data.csv','summary2.txt',3,'all','all','all'], 'expected':'new:43\\nlike:27\\nday:26\\nfeeling:26'},\n",
        "            {'args':['social_media_data.csv','summary.txt',5,'2023','USA','Facebook'], 'expected':'art:4\\nevery:3\\nexperience:3\\nexploring:3\\nlocal:3\\nlove:3'},\n",
        "            {'args':['social_media_data.csv','summary2.txt',3,'2023','USA','Facebook'], 'expected':'art:4\\nevery:3\\nexperience:3\\nexploring:3\\nlocal:3\\nlove:3'},\n",
        "            {'args':['social_media_data.csv','summary.txt',5,'all','Thailand','all'], 'expected':'#culinaryadventure:1\\n#streetfooddelights:1\\nadventure:1\\naromas:1\\nbustling:1\\nconnoisseur:1\\nculinary:1\\ndiverse:1\\nflavors:1\\nfood:1\\nheart:1\\nindulges:1\\nmarket:1\\nsavoring:1\\nstreet:1'},\n",
        "            {'args':['social_media_data.csv','summary1.txt',3,'2023','Thailand','all'], 'expected':'No data'},\n",
        "            {'args':['social_media_data.csv','summary2.txt',5,'2023','UK','Instagram'], 'expected':'new:7\\nadventure:4\\nfriends:4\\nart:3\\nday:3\\nmagic:3\\nweekend:3\\nwine:3\\nyears:3'},\n",
        "            {'args':['social_media_data.csv','summary3.txt',3,'all','India','all'], 'expected':'day:5\\nnew:5\\npainting:5'},\n",
        "            {'args':['social_media_data.csv','summary4.txt',5,'all','India','all'], 'expected':'day:5\\nnew:5\\npainting:5\\n#gratitude:4\\n#hopeful:4\\ndreams:4\\nhopeful:4\\nlife:4\\noptimism:4'},\n",
        "\n",
        "            ]\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            with self.subTest(i = i):\n",
        "                file_in, file_out, k, year, country, platform = test_case['args']\n",
        "                count_word_summary(file_in, file_out, k, year, country, platform)\n",
        "                f = open(file_out,'r',encoding=\"utf-8\")\n",
        "                self.assertEqual(\"\".join(f.readlines()).strip(), test_case['expected'])\n",
        "                f.close()\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtlurKcT_tfA",
        "outputId": "99dfaac7-b5b6-4048-d114-fac3f3726af0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_1_social_media_data (__main__.TestHW4) ... ok\n",
            "test_2_is_stopword (__main__.TestHW4) ... ok\n",
            "test_3_count_words_from_text (__main__.TestHW4) ... ok\n",
            "test_4_count_words_from_data_dict (__main__.TestHW4) ... ok\n",
            "test_5_top_k_words (__main__.TestHW4) ... ok\n",
            "test_6_count_word_summary (__main__.TestHW4) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 1.248s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7c74c800b100>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}